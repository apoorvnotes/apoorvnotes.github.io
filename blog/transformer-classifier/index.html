<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/494afaf338ab5c7d.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7c9a1029ba1c2cf7.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-9747723084e35360.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-a99b58d3cc150217.js" async=""></script><script src="/_next/static/chunks/596-ff6b9f15ce906e24.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><title>Text classification with Transformer</title><meta name="description" content="Implement transformer block as a Keras layer and use it for text classification."/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">↖ <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>Text classification with Transformer</h1><p class="text-neutral-500">10 May 2020</p><div><p>The example below shows implentation of a transformer block as a Keras layer, which can be used in place of an LSTM or GRU layer to process sequential input.
We take the mean of transformer outputs at each time step and use a feed forward network on top of it to classify text.</p>
<ul>
<li>The transformer shown in the example is a small one; 2 attention heads, and small dimensions for projection heads and FFN. It achieves validation accuracy of 87% on IMDB in less than a minute on CPU.  </li>
<li>The model trains faster than an LSTM based classifier with a similar number of parameters and gives slightly better results.  </li>
<li>Positional embedding are implemented using a standard keras.layers.Embedding layer, which according to the original paper produced nearly identical results to the sinusoidal version.</li>
</ul>
<h2>Setup</h2>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
</code></pre>
<h2>Implement multi head self attention as a Keras layer</h2>
<pre><code class="language-python">
class MultiHeadSelfAttention(keras.layers.Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        assert (
            embed_dim % num_heads == 0
        ), &quot;embedding dimension not divisible by num heads&quot;
        self.projection_dim = embed_dim // num_heads
        self.wq = keras.layers.Dense(embed_dim)
        self.wk = keras.layers.Dense(embed_dim)
        self.wv = keras.layers.Dense(embed_dim)
        self.combine_heads = keras.layers.Dense(embed_dim)

    def attention(self, q, k, v):
        score = tf.matmul(q, k, transpose_b=True)
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dk)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, v)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, x):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(x)[0]
        q = self.wq(x)  # (batch_size, seq_len, embed_dim)
        k = self.wk(x)  # (batch_size, seq_len, embed_dim)
        v = self.wv(x)  # (batch_size, seq_len, embed_dim)
        q = self.separate_heads(
            q, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        k = self.separate_heads(
            k, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        v = self.separate_heads(
            v, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(q, k, v)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output

</code></pre>
<h2>Implement transformer block as a layer</h2>
<pre><code class="language-python">
class TransformerLayer(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerLayer, self).__init__()

        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = keras.Sequential(
            [
                keras.layers.Dense(ff_dim, activation=&quot;relu&quot;),
                keras.layers.Dense(embed_dim),
            ]
        )

        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training):
        attn_output = self.att(x)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

</code></pre>
<h2>Implement embedding layer</h2>
<p>Two seperate embedding layers, one for tokens, one for token index (positions).</p>
<pre><code class="language-python">
class EmbeddingLayer(keras.layers.Layer):
    def __init__(self, maxlen, vocab_size, emded_dim):
        super(EmbeddingLayer, self).__init__()
        self.token_emb = keras.layers.Embedding(
            input_dim=vocab_size, output_dim=emded_dim
        )
        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=emded_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

</code></pre>
<h2>Create classifier model using transformer layer</h2>
<p>Transformer layer outputs one vector for each time step of your input sequence. Here, we take the mean across all time steps and build a two layered feed forward network on top of it.</p>
<pre><code class="language-python">
class TransformerClassifier(tf.keras.Model):
    def __init__(self, maxlen, vocab_size, embed_dim, ff_dim, num_heads):
        super(TransformerClassifier, self).__init__()
        self.emb = EmbeddingLayer(maxlen, vocab_size, embed_dim)
        self.transformer = TransformerLayer(embed_dim, num_heads, ff_dim)
        self.prehead = keras.layers.Dense(20, activation=&quot;relu&quot;)
        self.dropout1 = keras.layers.Dropout(0.05)
        self.dropout2 = keras.layers.Dropout(0.05)
        self.head = keras.layers.Dense(2, activation=&quot;softmax&quot;)

    def call(self, x, training):
        x = self.emb(x)
        x = self.transformer(x, training)
        x = tf.math.reduce_mean(x, axis=1)
        x = self.dropout1(x, training=training)
        x = self.prehead(x)
        x = self.dropout2(x, training=training)
        x = self.head(x)
        return x

</code></pre>
<h2>Download and prepare dataset</h2>
<pre><code class="language-python">max_features = 6000  # Only consider the top 20k words
maxlen = 200  # Only consider the first 200 words of each movie review
(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(
    num_words=max_features
)
print(len(x_train), &quot;Training sequences&quot;)
print(len(x_val), &quot;Validation sequences&quot;)
x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)
</code></pre>
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17465344/17464789 [==============================] - 2s 0us/step
25000 Training sequences
25000 Validation sequences
</code></pre>
<h2>Train and Evaluate</h2>
<pre><code class="language-python">model = TransformerClassifier(
    maxlen=maxlen, vocab_size=max_features, embed_dim=32, ff_dim=32, num_heads=1
)
model.compile(&quot;adam&quot;, &quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
history = model.fit(
    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)
)
</code></pre>
<pre><code>Epoch 1/2
782/782 [==============================] - 29s 37ms/step - loss: 0.3767 - accuracy: 0.8248 - val_loss: 0.3363 - val_accuracy: 0.8561
Epoch 2/2
782/782 [==============================] - 30s 39ms/step - loss: 0.2497 - accuracy: 0.8992 - val_loss: 0.2959 - val_accuracy: 0.8716
</code></pre>
</div></article></div></main><script src="/_next/static/chunks/webpack-9747723084e35360.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/494afaf338ab5c7d.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/7c9a1029ba1c2cf7.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":7767,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"5"])</script><script>self.__next_f.push([1,"96:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\na:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/494afaf338ab5c7d.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"KnsO8z-ENGbRNQP4VRChH\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/transformer-classifier/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"transformer-classifier\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"transformer-classifier\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"transformer-classifier\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",\"$Lc\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"transformer-classifier\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7c9a1029ba1c2cf7.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"transformer-classifier\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"d:T1e92,"])</script><script>self.__next_f.push([1,"\u003cp\u003eThe example below shows implentation of a transformer block as a Keras layer, which can be used in place of an LSTM or GRU layer to process sequential input.\nWe take the mean of transformer outputs at each time step and use a feed forward network on top of it to classify text.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe transformer shown in the example is a small one; 2 attention heads, and small dimensions for projection heads and FFN. It achieves validation accuracy of 87% on IMDB in less than a minute on CPU.  \u003c/li\u003e\n\u003cli\u003eThe model trains faster than an LSTM based classifier with a similar number of parameters and gives slightly better results.  \u003c/li\u003e\n\u003cli\u003ePositional embedding are implemented using a standard keras.layers.Embedding layer, which according to the original paper produced nearly identical results to the sinusoidal version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSetup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport tensorflow as tf\nfrom tensorflow import keras\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eImplement multi head self attention as a Keras layer\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nclass MultiHeadSelfAttention(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert (\n            embed_dim % num_heads == 0\n        ), \u0026quot;embedding dimension not divisible by num heads\u0026quot;\n        self.projection_dim = embed_dim // num_heads\n        self.wq = keras.layers.Dense(embed_dim)\n        self.wk = keras.layers.Dense(embed_dim)\n        self.wv = keras.layers.Dense(embed_dim)\n        self.combine_heads = keras.layers.Dense(embed_dim)\n\n    def attention(self, q, k, v):\n        score = tf.matmul(q, k, transpose_b=True)\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dk)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, v)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, x):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(x)[0]\n        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n        q = self.separate_heads(\n            q, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        k = self.separate_heads(\n            k, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        v = self.separate_heads(\n            v, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(q, k, v)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eImplement transformer block as a layer\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nclass TransformerLayer(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerLayer, self).__init__()\n\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [\n                keras.layers.Dense(ff_dim, activation=\u0026quot;relu\u0026quot;),\n                keras.layers.Dense(embed_dim),\n            ]\n        )\n\n        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training):\n        attn_output = self.att(x)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eImplement embedding layer\u003c/h2\u003e\n\u003cp\u003eTwo seperate embedding layers, one for tokens, one for token index (positions).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nclass EmbeddingLayer(keras.layers.Layer):\n    def __init__(self, maxlen, vocab_size, emded_dim):\n        super(EmbeddingLayer, self).__init__()\n        self.token_emb = keras.layers.Embedding(\n            input_dim=vocab_size, output_dim=emded_dim\n        )\n        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreate classifier model using transformer layer\u003c/h2\u003e\n\u003cp\u003eTransformer layer outputs one vector for each time step of your input sequence. Here, we take the mean across all time steps and build a two layered feed forward network on top of it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nclass TransformerClassifier(tf.keras.Model):\n    def __init__(self, maxlen, vocab_size, embed_dim, ff_dim, num_heads):\n        super(TransformerClassifier, self).__init__()\n        self.emb = EmbeddingLayer(maxlen, vocab_size, embed_dim)\n        self.transformer = TransformerLayer(embed_dim, num_heads, ff_dim)\n        self.prehead = keras.layers.Dense(20, activation=\u0026quot;relu\u0026quot;)\n        self.dropout1 = keras.layers.Dropout(0.05)\n        self.dropout2 = keras.layers.Dropout(0.05)\n        self.head = keras.layers.Dense(2, activation=\u0026quot;softmax\u0026quot;)\n\n    def call(self, x, training):\n        x = self.emb(x)\n        x = self.transformer(x, training)\n        x = tf.math.reduce_mean(x, axis=1)\n        x = self.dropout1(x, training=training)\n        x = self.prehead(x)\n        x = self.dropout2(x, training=training)\n        x = self.head(x)\n        return x\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDownload and prepare dataset\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003emax_features = 6000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\n(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n    num_words=max_features\n)\nprint(len(x_train), \u0026quot;Training sequences\u0026quot;)\nprint(len(x_val), \u0026quot;Validation sequences\u0026quot;)\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 2s 0us/step\n25000 Training sequences\n25000 Validation sequences\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTrain and Evaluate\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003emodel = TransformerClassifier(\n    maxlen=maxlen, vocab_size=max_features, embed_dim=32, ff_dim=32, num_heads=1\n)\nmodel.compile(\u0026quot;adam\u0026quot;, \u0026quot;sparse_categorical_crossentropy\u0026quot;, metrics=[\u0026quot;accuracy\u0026quot;])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eEpoch 1/2\n782/782 [==============================] - 29s 37ms/step - loss: 0.3767 - accuracy: 0.8248 - val_loss: 0.3363 - val_accuracy: 0.8561\nEpoch 2/2\n782/782 [==============================] - 30s 39ms/step - loss: 0.2497 - accuracy: 0.8992 - val_loss: 0.2959 - val_accuracy: 0.8716\n\u003c/code\u003e\u003c/pre\u003e\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"↖ \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Text classification with Transformer\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"10 May 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$d\"}}]]}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Text classification with Transformer\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Implement transformer block as a Keras layer and use it for text classification.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nb:null\n"])</script></body></html>