<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/494afaf338ab5c7d.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7c9a1029ba1c2cf7.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-9747723084e35360.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-a99b58d3cc150217.js" async=""></script><script src="/_next/static/chunks/596-ff6b9f15ce906e24.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><title>Actor Critic Method</title><meta name="description" content="Implement Actor Critic Method in CartPole environment."/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">â†– <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>Actor Critic Method</h1><p class="text-neutral-500">13 May 2020</p><div><h2>Introduction</h2>
<p>An implementation of Actor Critic method on CartPole-V0 environment.</p>
<h3>Actor Critic Method</h3>
<p>As an agent takes actions and moves through an environment, it learns to map
the observed state of the environment to two possible outputs:</p>
<ol>
<li>Recommended action: A probabiltiy value for each action in the action space.
The part of the agent responsible for this output is called the <strong>actor</strong>.</li>
<li>Estimated rewards in the future: Sum of all rewards it expects to receive in the
future. The part of the agent responsible for this output is the <strong>critic</strong>.</li>
</ol>
<p>Agent and Critic learn to perform their tasks, such that the recommended actions
from the actor maximize the rewards.</p>
<h3>CartPole-V0</h3>
<p>A pole is attached to a cart placed on a frictionless track. The agent has to apply
force to move the cart. It is rewarded for for every time step the pole
remains upright. The agent, therefore, must learn to keep the pole from falling over.</p>
<hr>
<pre><code class="language-python">import gym
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Configuration paramaters for the whole setup
seed = 42
gamma = 0.99  # Discount factor for past rewards
max_steps_per_episode = 10000
env = gym.make(&quot;CartPole-v0&quot;)  # Create the environment
env.seed(seed)
eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0
</code></pre>
<h2>Implement Actor Critic network</h2>
<p>This network learns two functions:</p>
<ol>
<li>Actor: This takes as input the state of our environment and returns a
probability value for each action in its action space.</li>
<li>Critic: This takes as input the state of our environment and returns
an estimate of total rewards in the future.</li>
</ol>
<p>In our implementation, they share the initial layer.</p>
<pre><code class="language-python">num_inputs = 4
num_actions = 2
num_hidden = 128

inputs = layers.Input(shape=(num_inputs,))
common = layers.Dense(num_hidden, activation=&quot;relu&quot;)(inputs)
action = layers.Dense(num_actions, activation=&quot;softmax&quot;)(common)
critic = layers.Dense(1)(common)

model = keras.Model(inputs=inputs, outputs=[action, critic])
</code></pre>
<h2>Train</h2>
<pre><code class="language-python">optimizer = keras.optimizers.Adam(learning_rate=0.01)
huber_loss = keras.losses.Huber()
action_probs_history = []
critic_value_history = []
rewards_history = []
running_reward = 0
episode_count = 0

while True:  # Run until solved
    state = env.reset()
    episode_reward = 0
    with tf.GradientTape() as tape:
        for timestep in range(1, max_steps_per_episode):
            # env.render(); Adding this line would show the attempts
            # of the agent in a pop up window.

            state = tf.convert_to_tensor(state)
            state = tf.expand_dims(state, 0)

            # Predict action probabilities and estimated future rewards
            # from environment state
            action_probs, critic_value = model(state)
            critic_value_history.append(critic_value[0, 0])

            # Sample action from action probability distribution
            action = np.random.choice(num_actions, p=np.squeeze(action_probs))
            action_probs_history.append(tf.math.log(action_probs[0, action]))

            # Apply the sampled action in our environment
            state, reward, done, _ = env.step(action)
            rewards_history.append(reward)
            episode_reward += reward

            if done:
                break

        # Update running reward to check condition for solving
        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward

        # Calculate expected value from rewards
        # - At each timestep what was the total reward received after that timestep
        # - Rewards in the past are discounted by multiplying them with gamma
        # - These are the labels for our critic
        returns = []
        discounted_sum = 0
        for r in rewards_history[::-1]:
            discounted_sum = r + gamma * discounted_sum
            returns.insert(0, discounted_sum)

        # Normalize
        returns = np.array(returns)
        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)
        returns = returns.tolist()

        # Calculating loss values to update our network
        history = zip(action_probs_history, critic_value_history, returns)
        actor_losses = []
        critic_losses = []
        for log_prob, value, ret in history:
            # At this point in history, the critic estimated that we would get a
            # total reward = `value` in the future. We took an action with log probability
            # of `log_prob` and ended up recieving a total reward = `ret`.
            # The actor must be updated so that it predicts an action that leads to
            # high rewards (compared to critic&#39;s estimate) with high probability.
            diff = ret - value
            actor_losses.append(-log_prob * diff)  # actor loss

            # The critic must be updated so that it predicts a better estimate of
            # the future rewards.
            critic_losses.append(
                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))
            )

        # Backpropagation
        loss_value = sum(actor_losses) + sum(critic_losses)
        grads = tape.gradient(loss_value, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Clear the loss and reward history
        action_probs_history.clear()
        critic_value_history.clear()
        rewards_history.clear()

    # Log details
    episode_count += 1
    if episode_count % 10 == 0:
        template = &quot;running reward: {:.2f} at episode {}&quot;
        print(template.format(running_reward, episode_count))

    if running_reward &gt; 195:  # Condition to consider the task solved
        print(&quot;Solved at episode {}!&quot;.format(episode_count))
        break
</code></pre>
<h2>Visualizations</h2>
<p>In early stages of training:<br><img src="https://i.imgur.com/5gCs5kH.gif" alt="Imgur"></p>
<p>In later stages of training:<br><img src="https://i.imgur.com/5ziiZUD.gif" alt="Imgur"></p>
<h2>References</h2>
<ul>
<li><a href="http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf">CartPole</a></li>
<li><a href="https://hal.inria.fr/hal-00840470/document">Actor Critic Method</a></li>
</ul>
</div></article></div></main><script src="/_next/static/chunks/webpack-9747723084e35360.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/494afaf338ab5c7d.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/7c9a1029ba1c2cf7.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":7767,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"5"])</script><script>self.__next_f.push([1,"96:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\na:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/494afaf338ab5c7d.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"KnsO8z-ENGbRNQP4VRChH\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/actor-critic/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"actor-critic\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"actor-critic\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"actor-critic\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",\"$Lc\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"actor-critic\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7c9a1029ba1c2cf7.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"actor-critic\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Actor Critic Method\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Implement Actor Critic Method in CartPole environment.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nb:null\nd:T197f,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAn implementation of Actor Critic method on CartPole-V0 environment.\u003c/p\u003e\n\u003ch3\u003eActor Critic Method\u003c/h3\u003e\n\u003cp\u003eAs an agent takes actions and moves through an environment, it learns to map\nthe observed state of the environment to two possible outputs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRecommended action: A probabiltiy value for each action in the action space.\nThe part of the agent responsible for this output is called the \u003cstrong\u003eactor\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eEstimated rewards in the future: Sum of all rewards it expects to receive in the\nfuture. The part of the agent responsible for this output is the \u003cstrong\u003ecritic\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAgent and Critic learn to perform their tasks, such that the recommended actions\nfrom the actor maximize the rewards.\u003c/p\u003e\n\u003ch3\u003eCartPole-V0\u003c/h3\u003e\n\u003cp\u003eA pole is attached to a cart placed on a frictionless track. The agent has to apply\nforce to move the cart. It is rewarded for for every time step the pole\nremains upright. The agent, therefore, must learn to keep the pole from falling over.\u003c/p\u003e\n\u003chr\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Configuration paramaters for the whole setup\nseed = 42\ngamma = 0.99  # Discount factor for past rewards\nmax_steps_per_episode = 10000\nenv = gym.make(\u0026quot;CartPole-v0\u0026quot;)  # Create the environment\nenv.seed(seed)\neps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eImplement Actor Critic network\u003c/h2\u003e\n\u003cp\u003eThis network learns two functions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eActor: This takes as input the state of our environment and returns a\nprobability value for each action in its action space.\u003c/li\u003e\n\u003cli\u003eCritic: This takes as input the state of our environment and returns\nan estimate of total rewards in the future.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn our implementation, they share the initial layer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003enum_inputs = 4\nnum_actions = 2\nnum_hidden = 128\n\ninputs = layers.Input(shape=(num_inputs,))\ncommon = layers.Dense(num_hidden, activation=\u0026quot;relu\u0026quot;)(inputs)\naction = layers.Dense(num_actions, activation=\u0026quot;softmax\u0026quot;)(common)\ncritic = layers.Dense(1)(common)\n\nmodel = keras.Model(inputs=inputs, outputs=[action, critic])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTrain\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eoptimizer = keras.optimizers.Adam(learning_rate=0.01)\nhuber_loss = keras.losses.Huber()\naction_probs_history = []\ncritic_value_history = []\nrewards_history = []\nrunning_reward = 0\nepisode_count = 0\n\nwhile True:  # Run until solved\n    state = env.reset()\n    episode_reward = 0\n    with tf.GradientTape() as tape:\n        for timestep in range(1, max_steps_per_episode):\n            # env.render(); Adding this line would show the attempts\n            # of the agent in a pop up window.\n\n            state = tf.convert_to_tensor(state)\n            state = tf.expand_dims(state, 0)\n\n            # Predict action probabilities and estimated future rewards\n            # from environment state\n            action_probs, critic_value = model(state)\n            critic_value_history.append(critic_value[0, 0])\n\n            # Sample action from action probability distribution\n            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n            action_probs_history.append(tf.math.log(action_probs[0, action]))\n\n            # Apply the sampled action in our environment\n            state, reward, done, _ = env.step(action)\n            rewards_history.append(reward)\n            episode_reward += reward\n\n            if done:\n                break\n\n        # Update running reward to check condition for solving\n        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n        # Calculate expected value from rewards\n        # - At each timestep what was the total reward received after that timestep\n        # - Rewards in the past are discounted by multiplying them with gamma\n        # - These are the labels for our critic\n        returns = []\n        discounted_sum = 0\n        for r in rewards_history[::-1]:\n            discounted_sum = r + gamma * discounted_sum\n            returns.insert(0, discounted_sum)\n\n        # Normalize\n        returns = np.array(returns)\n        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n        returns = returns.tolist()\n\n        # Calculating loss values to update our network\n        history = zip(action_probs_history, critic_value_history, returns)\n        actor_losses = []\n        critic_losses = []\n        for log_prob, value, ret in history:\n            # At this point in history, the critic estimated that we would get a\n            # total reward = `value` in the future. We took an action with log probability\n            # of `log_prob` and ended up recieving a total reward = `ret`.\n            # The actor must be updated so that it predicts an action that leads to\n            # high rewards (compared to critic\u0026#39;s estimate) with high probability.\n            diff = ret - value\n            actor_losses.append(-log_prob * diff)  # actor loss\n\n            # The critic must be updated so that it predicts a better estimate of\n            # the future rewards.\n            critic_losses.append(\n                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n            )\n\n        # Backpropagation\n        loss_value = sum(actor_losses) + sum(critic_losses)\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        # Clear the loss and reward history\n        action_probs_history.clear()\n        critic_value_history.clear()\n        rewards_history.clear()\n\n    # Log details\n    episode_count += 1\n    if episode_count % 10 == 0:\n        template = \u0026quot;running reward: {:.2f} at episode {}\u0026quot;\n        print(template.format(running_reward, episode_count))\n\n    if running_reward \u0026gt; 195:  # Condition to consider the task solved\n        print(\u0026quot;Solved at episode {}!\u0026quot;.format(episode_count))\n        break\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eVisualizations\u003c/h2\u003e\n\u003cp\u003eIn early stages of training:\u003cbr\u003e\u003cimg src=\"https://i.imgur.com/5gCs5kH.gif\" alt=\"Imgur\"\u003e\u003c/p\u003e\n\u003cp\u003eIn later stages of training:\u003cbr\u003e\u003cimg src=\"https://i.imgur.com/5ziiZUD.gif\" alt=\"Imgur\"\u003e\u003c/p\u003e\n\u003ch2\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf\"\u003eCartPole\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://hal.inria.fr/hal-00840470/document\"\u003eActor Critic Method\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"â†– \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Actor Critic Method\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"13 May 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$d\"}}]]}]]\n"])</script></body></html>