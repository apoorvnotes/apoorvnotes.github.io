<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/494afaf338ab5c7d.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7c9a1029ba1c2cf7.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-9747723084e35360.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-a99b58d3cc150217.js" async=""></script><script src="/_next/static/chunks/596-ff6b9f15ce906e24.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><title>Model Implementation</title><meta name="description" content="Using Keras&#x27; Functional API to implement U-Net architecture"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">â†– <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>Model Implementation</h1><p class="text-neutral-500">30 August 2020</p><div><h2>Introduction</h2>
<p>Original paper can be found <a href="https://arxiv.org/abs/1505.04597">here</a>.<br>
U-net predicts a class label for each input pixel.  The architecture is fully convolutional and is shown to perform well with small datasets for image segmentation 
tasks (especially for biomedical images).</p>
<p>I&#39;ll follow the exact architecture given in the Figure 1. from the paper (copied below)
<image src="/assets/unet.png" style="width:100%;"></p>
<h2>Implementation in 50 lines of code</h2>
<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model

def encode(inputs):
    conv1 = layers.Conv2D(64, 3, activation = &#39;relu&#39;)(inputs)
    conv1 = layers.Conv2D(64, 3, activation = &#39;relu&#39;)(conv1)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = layers.Conv2D(128, 3, activation = &#39;relu&#39;)(pool1)
    conv2 = layers.Conv2D(128, 3, activation = &#39;relu&#39;)(conv2)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = layers.Conv2D(256, 3, activation = &#39;relu&#39;)(pool2)
    conv3 = layers.Conv2D(256, 3, activation = &#39;relu&#39;)(conv3)
    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = layers.Conv2D(512, 3, activation = &#39;relu&#39;)(pool3)
    conv4 = layers.Conv2D(512, 3, activation = &#39;relu&#39;)(conv4)
    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)
    conv5 = layers.Conv2D(1024, 3, activation = &#39;relu&#39;)(pool4)
    conv5 = layers.Conv2D(1024, 3, activation = &#39;relu&#39;)(conv5)
    return conv5, conv4, conv3, conv2, conv1

def decode(conv5, conv4, conv3, conv2, conv1, num_classes):
    up6 = layers.Conv2DTranspose(512, 2, strides=(2, 2))(conv5)
    crop4 = layers.Cropping2D(4)(conv4)
    concat6 = layers.Concatenate(axis=3)([crop4,up6])
    conv6 = layers.Conv2D(512, 3, activation = &#39;relu&#39;)(concat6)
    conv6 = layers.Conv2D(512, 3, activation = &#39;relu&#39;)(conv6)

    up7 = layers.Conv2DTranspose(256, 2, strides=(2, 2))(conv6)
    crop3 = layers.Cropping2D(16)(conv3)
    concat7 = layers.Concatenate(axis=3)([crop3,up7])
    conv7 = layers.Conv2D(256, 3, activation = &#39;relu&#39;)(concat7)
    conv7 = layers.Conv2D(256, 3, activation = &#39;relu&#39;)(conv7)

    up8 = layers.Conv2DTranspose(128, 2, strides=(2, 2))(conv7)
    crop2 = layers.Cropping2D(40)(conv2)
    concat8 = layers.Concatenate(axis=3)([crop2,up8])
    conv8 = layers.Conv2D(128, 3, activation = &#39;relu&#39;)(concat8)
    conv8 = layers.Conv2D(128, 3, activation = &#39;relu&#39;)(conv8)

    up9 = layers.Conv2DTranspose(64, 2, strides=(2, 2))(conv8)
    crop1 = layers.Cropping2D(88)(conv1)
    concat9 = layers.Concatenate(axis=3)([crop1,up9])
    conv9 = layers.Conv2D(64, 3, activation = &#39;relu&#39;)(concat9)
    conv9 = layers.Conv2D(64, 3, activation = &#39;relu&#39;)(conv9)
    conv10 = layers.Conv2D(num_classes, 1)(conv9)
    conv10 = layers.Softmax(axis=-1)(conv10)
    return conv10

def create_unet(input_size=(572,572,1), num_classes=2):
    inputs = layers.Input(input_size)
    conv5, conv4, conv3, conv2, conv1 = encode(inputs)
    conv10 = decode(conv5, conv4, conv3, conv2, conv1, num_classes)
    model = Model(inputs, conv10)
    model.compile(optimizer = Adam(lr=1e-4), loss=&#39;categorical_crossentropy&#39;)
    return model

model = create_unet()
model.summary()
</code></pre>
<pre><code class="language-text">__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 572, 572, 1) 0                                            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 570, 570, 64) 640         input_2[0][0]                    
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 568, 568, 64) 36928       conv2d_19[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 284, 284, 64) 0           conv2d_20[0][0]                  
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 282, 282, 128 73856       max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 280, 280, 128 147584      conv2d_21[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 140, 140, 128 0           conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 138, 138, 256 295168      max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 136, 136, 256 590080      conv2d_23[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 68, 68, 256)  0           conv2d_24[0][0]                  
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 66, 66, 512)  1180160     max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_25[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_26[0][0]                  
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 30, 30, 1024) 4719616     max_pooling2d_7[0][0]            
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 28, 28, 1024) 9438208     conv2d_27[0][0]                  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 56, 56, 512)  0           conv2d_26[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 56, 56, 512)  2097664     conv2d_28[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 56, 1024) 0           cropping2d_4[0][0]               
                                                                 conv2d_transpose_4[0][0]         
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 54, 54, 512)  4719104     concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 52, 52, 512)  2359808     conv2d_29[0][0]                  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 104, 104, 256 0           conv2d_24[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_5 (Conv2DTrans (None, 104, 104, 256 524544      conv2d_30[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 104, 104, 512 0           cropping2d_5[0][0]               
                                                                 conv2d_transpose_5[0][0]         
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 102, 102, 256 1179904     concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 100, 100, 256 590080      conv2d_31[0][0]                  
__________________________________________________________________________________________________
cropping2d_6 (Cropping2D)       (None, 200, 200, 128 0           conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_6 (Conv2DTrans (None, 200, 200, 128 131200      conv2d_32[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 200, 200, 256 0           cropping2d_6[0][0]               
                                                                 conv2d_transpose_6[0][0]         
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 198, 198, 128 295040      concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 196, 196, 128 147584      conv2d_33[0][0]                  
__________________________________________________________________________________________________
cropping2d_7 (Cropping2D)       (None, 392, 392, 64) 0           conv2d_20[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_7 (Conv2DTrans (None, 392, 392, 64) 32832       conv2d_34[0][0]                  
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 392, 392, 128 0           cropping2d_7[0][0]               
                                                                 conv2d_transpose_7[0][0]         
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 390, 390, 64) 73792       concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 388, 388, 64) 36928       conv2d_35[0][0]                  
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 388, 388, 2)  130         conv2d_36[0][0]                  
__________________________________________________________________________________________________
softmax (Softmax)               (None, 388, 388, 2)  0           conv2d_37[0][0]                  
==================================================================================================
Total params: 31,030,658
Trainable params: 31,030,658
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
</div></article></div></main><script src="/_next/static/chunks/webpack-9747723084e35360.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/494afaf338ab5c7d.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/7c9a1029ba1c2cf7.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":7767,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"5"])</script><script>self.__next_f.push([1,"96:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\na:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/494afaf338ab5c7d.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"KnsO8z-ENGbRNQP4VRChH\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/unet-keras/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"unet-keras\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"unet-keras\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"unet-keras\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",\"$Lc\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"unet-keras\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7c9a1029ba1c2cf7.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"unet-keras\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Model Implementation\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Using Keras' Functional API to implement U-Net architecture\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nb:null\nd:T2dc1,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eOriginal paper can be found \u003ca href=\"https://arxiv.org/abs/1505.04597\"\u003ehere\u003c/a\u003e.\u003cbr\u003e\nU-net predicts a class label for each input pixel.  The architecture is fully convolutional and is shown to perform well with small datasets for image segmentation \ntasks (especially for biomedical images).\u003c/p\u003e\n\u003cp\u003eI\u0026#39;ll follow the exact architecture given in the Figure 1. from the paper (copied below)\n\u003cimage src=\"/assets/unet.png\" style=\"width:100%;\"\u003e\u003c/p\u003e\n\u003ch2\u003eImplementation in 50 lines of code\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Model\n\ndef encode(inputs):\n    conv1 = layers.Conv2D(64, 3, activation = \u0026#39;relu\u0026#39;)(inputs)\n    conv1 = layers.Conv2D(64, 3, activation = \u0026#39;relu\u0026#39;)(conv1)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = layers.Conv2D(128, 3, activation = \u0026#39;relu\u0026#39;)(pool1)\n    conv2 = layers.Conv2D(128, 3, activation = \u0026#39;relu\u0026#39;)(conv2)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = layers.Conv2D(256, 3, activation = \u0026#39;relu\u0026#39;)(pool2)\n    conv3 = layers.Conv2D(256, 3, activation = \u0026#39;relu\u0026#39;)(conv3)\n    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = layers.Conv2D(512, 3, activation = \u0026#39;relu\u0026#39;)(pool3)\n    conv4 = layers.Conv2D(512, 3, activation = \u0026#39;relu\u0026#39;)(conv4)\n    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n    conv5 = layers.Conv2D(1024, 3, activation = \u0026#39;relu\u0026#39;)(pool4)\n    conv5 = layers.Conv2D(1024, 3, activation = \u0026#39;relu\u0026#39;)(conv5)\n    return conv5, conv4, conv3, conv2, conv1\n\ndef decode(conv5, conv4, conv3, conv2, conv1, num_classes):\n    up6 = layers.Conv2DTranspose(512, 2, strides=(2, 2))(conv5)\n    crop4 = layers.Cropping2D(4)(conv4)\n    concat6 = layers.Concatenate(axis=3)([crop4,up6])\n    conv6 = layers.Conv2D(512, 3, activation = \u0026#39;relu\u0026#39;)(concat6)\n    conv6 = layers.Conv2D(512, 3, activation = \u0026#39;relu\u0026#39;)(conv6)\n\n    up7 = layers.Conv2DTranspose(256, 2, strides=(2, 2))(conv6)\n    crop3 = layers.Cropping2D(16)(conv3)\n    concat7 = layers.Concatenate(axis=3)([crop3,up7])\n    conv7 = layers.Conv2D(256, 3, activation = \u0026#39;relu\u0026#39;)(concat7)\n    conv7 = layers.Conv2D(256, 3, activation = \u0026#39;relu\u0026#39;)(conv7)\n\n    up8 = layers.Conv2DTranspose(128, 2, strides=(2, 2))(conv7)\n    crop2 = layers.Cropping2D(40)(conv2)\n    concat8 = layers.Concatenate(axis=3)([crop2,up8])\n    conv8 = layers.Conv2D(128, 3, activation = \u0026#39;relu\u0026#39;)(concat8)\n    conv8 = layers.Conv2D(128, 3, activation = \u0026#39;relu\u0026#39;)(conv8)\n\n    up9 = layers.Conv2DTranspose(64, 2, strides=(2, 2))(conv8)\n    crop1 = layers.Cropping2D(88)(conv1)\n    concat9 = layers.Concatenate(axis=3)([crop1,up9])\n    conv9 = layers.Conv2D(64, 3, activation = \u0026#39;relu\u0026#39;)(concat9)\n    conv9 = layers.Conv2D(64, 3, activation = \u0026#39;relu\u0026#39;)(conv9)\n    conv10 = layers.Conv2D(num_classes, 1)(conv9)\n    conv10 = layers.Softmax(axis=-1)(conv10)\n    return conv10\n\ndef create_unet(input_size=(572,572,1), num_classes=2):\n    inputs = layers.Input(input_size)\n    conv5, conv4, conv3, conv2, conv1 = encode(inputs)\n    conv10 = decode(conv5, conv4, conv3, conv2, conv1, num_classes)\n    model = Model(inputs, conv10)\n    model.compile(optimizer = Adam(lr=1e-4), loss=\u0026#39;categorical_crossentropy\u0026#39;)\n    return model\n\nmodel = create_unet()\nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-text\"\u003e__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 572, 572, 1) 0                                            \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 570, 570, 64) 640         input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 568, 568, 64) 36928       conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 284, 284, 64) 0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 282, 282, 128 73856       max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 280, 280, 128 147584      conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 140, 140, 128 0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 138, 138, 256 295168      max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 136, 136, 256 590080      conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 68, 68, 256)  0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 66, 66, 512)  1180160     max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 30, 30, 1024) 4719616     max_pooling2d_7[0][0]            \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 28, 28, 1024) 9438208     conv2d_27[0][0]                  \n__________________________________________________________________________________________________\ncropping2d_4 (Cropping2D)       (None, 56, 56, 512)  0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 56, 56, 512)  2097664     conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 56, 56, 1024) 0           cropping2d_4[0][0]               \n                                                                 conv2d_transpose_4[0][0]         \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 54, 54, 512)  4719104     concatenate_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 52, 52, 512)  2359808     conv2d_29[0][0]                  \n__________________________________________________________________________________________________\ncropping2d_5 (Cropping2D)       (None, 104, 104, 256 0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_5 (Conv2DTrans (None, 104, 104, 256 524544      conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 104, 104, 512 0           cropping2d_5[0][0]               \n                                                                 conv2d_transpose_5[0][0]         \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 102, 102, 256 1179904     concatenate_5[0][0]              \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 100, 100, 256 590080      conv2d_31[0][0]                  \n__________________________________________________________________________________________________\ncropping2d_6 (Cropping2D)       (None, 200, 200, 128 0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_6 (Conv2DTrans (None, 200, 200, 128 131200      conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 200, 200, 256 0           cropping2d_6[0][0]               \n                                                                 conv2d_transpose_6[0][0]         \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 198, 198, 128 295040      concatenate_6[0][0]              \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 196, 196, 128 147584      conv2d_33[0][0]                  \n__________________________________________________________________________________________________\ncropping2d_7 (Cropping2D)       (None, 392, 392, 64) 0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_7 (Conv2DTrans (None, 392, 392, 64) 32832       conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 392, 392, 128 0           cropping2d_7[0][0]               \n                                                                 conv2d_transpose_7[0][0]         \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 390, 390, 64) 73792       concatenate_7[0][0]              \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 388, 388, 64) 36928       conv2d_35[0][0]                  \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 388, 388, 2)  130         conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nsoftmax (Softmax)               (None, 388, 388, 2)  0           conv2d_37[0][0]                  \n==================================================================================================\nTotal params: 31,030,658\nTrainable params: 31,030,658\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\u003c/code\u003e\u003c/pre\u003e\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"â†– \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Model Implementation\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"30 August 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$d\"}}]]}]]\n"])</script></body></html>