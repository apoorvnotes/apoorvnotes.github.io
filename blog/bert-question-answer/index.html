<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/5b284c7119f7d307.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5f679fa11fe6d7f1.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-1dd3b446fbaabb6c.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-44fe9588d0e5e7bc.js" async=""></script><script src="/_next/static/chunks/596-27146c691e1092ff.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=G-M55V0FS97Q"/><title>BERT (from HuggingFace Transformers) for Text Extraction</title><meta name="description" content="Fine tune pretrained BERT from HuggingFace Transformers on SQuAD."/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">â†– <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>BERT (from HuggingFace Transformers) for Text Extraction</h1><p class="text-neutral-500">23 May 2020</p><div><p>Copy of this <a href="https://keras.io/examples/nlp/text_extraction_with_bert/">example</a> I wrote in Keras docs.</p>
<h2>Introduction</h2>
<p>This demonstration uses SQuAD (Stanford Question-Answering Dataset).
In SQuAD, an input consists of a question, and a paragraph for context.
The goal is to find the span of text in the paragraph that answers the question.
We evaluate our performance on this data with the &quot;Exact Match&quot; metric,
which measures the percentage of predictions that exactly match any one of the
ground-truth answers.</p>
<p>We fine-tune a BERT model to perform this task as follows:</p>
<ol>
<li>Feed the context and the question as inputs to BERT.</li>
<li>Take two vectors S and T with dimensions equal to that of
hidden states in BERT.</li>
<li>Compute the probability of each token being the start and end of
the answer span. The probability of a token being the start of
the answer is given by a dot product between S and the representation
of the token in the last layer of BERT, followed by a softmax over all tokens.
The probability of a token being the end of the answer is computed
similarly with the vector T.</li>
<li>Fine-tune BERT and learn S and T along the way.</li>
</ol>
<p><strong>References:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a></li>
<li><a href="https://arxiv.org/abs/1606.05250">SQuAD</a></li>
</ul>
<h2>Setup</h2>
<pre><code class="language-python">import os
import re
import json
import string
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig

max_len = 384
configuration = BertConfig()  # default parameters and configuration for BERT
</code></pre>
<hr>
<h2>Set-up BERT tokenizer</h2>
<pre><code class="language-python"># Save the slow pretrained tokenizer
slow_tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
save_path = &quot;bert_base_uncased/&quot;
if not os.path.exists(save_path):
    os.makedirs(save_path)
slow_tokenizer.save_pretrained(save_path)

# Load the fast tokenizer from saved file
tokenizer = BertWordPieceTokenizer(&quot;bert_base_uncased/vocab.txt&quot;, lowercase=True)
</code></pre>
<hr>
<h2>Load the data</h2>
<pre><code class="language-python">train_data_url = &quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json&quot;
train_path = keras.utils.get_file(&quot;train.json&quot;, train_data_url)
eval_data_url = &quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json&quot;
eval_path = keras.utils.get_file(&quot;eval.json&quot;, eval_data_url)
</code></pre>
<hr>
<h2>Preprocess the data</h2>
<ol>
<li>Go through the JSON file and store every record as a <code>SquadExample</code> object.</li>
<li>Go through each <code>SquadExample</code> and create <code>x_train, y_train, x_eval, y_eval</code>.</li>
</ol>
<pre><code class="language-python">
class SquadExample:
    def __init__(self, question, context, start_char_idx, answer_text, all_answers):
        self.question = question
        self.context = context
        self.start_char_idx = start_char_idx
        self.answer_text = answer_text
        self.all_answers = all_answers
        self.skip = False

    def preprocess(self):
        context = self.context
        question = self.question
        answer_text = self.answer_text
        start_char_idx = self.start_char_idx

        # Clean context, answer and question
        context = &quot; &quot;.join(str(context).split())
        question = &quot; &quot;.join(str(question).split())
        answer = &quot; &quot;.join(str(answer_text).split())

        # Find end character index of answer in context
        end_char_idx = start_char_idx + len(answer)
        if end_char_idx &gt;= len(context):
            self.skip = True
            return

        # Mark the character indexes in context that are in answer
        is_char_in_ans = [0] * len(context)
        for idx in range(start_char_idx, end_char_idx):
            is_char_in_ans[idx] = 1

        # Tokenize context
        tokenized_context = tokenizer.encode(context)

        # Find tokens that were created from answer characters
        ans_token_idx = []
        for idx, (start, end) in enumerate(tokenized_context.offsets):
            if sum(is_char_in_ans[start:end]) &gt; 0:
                ans_token_idx.append(idx)

        if len(ans_token_idx) == 0:
            self.skip = True
            return

        # Find start and end token index for tokens from answer
        start_token_idx = ans_token_idx[0]
        end_token_idx = ans_token_idx[-1]

        # Tokenize question
        tokenized_question = tokenizer.encode(question)

        # Create inputs
        input_ids = tokenized_context.ids + tokenized_question.ids[1:]
        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(
            tokenized_question.ids[1:]
        )
        attention_mask = [1] * len(input_ids)

        # Pad and create attention masks.
        # Skip if truncation is needed
        padding_length = max_len - len(input_ids)
        if padding_length &gt; 0:  # pad
            input_ids = input_ids + ([0] * padding_length)
            attention_mask = attention_mask + ([0] * padding_length)
            token_type_ids = token_type_ids + ([0] * padding_length)
        elif padding_length &lt; 0:  # skip
            self.skip = True
            return

        self.input_ids = input_ids
        self.token_type_ids = token_type_ids
        self.attention_mask = attention_mask
        self.start_token_idx = start_token_idx
        self.end_token_idx = end_token_idx
        self.context_token_to_char = tokenized_context.offsets


with open(train_path) as f:
    raw_train_data = json.load(f)

with open(eval_path) as f:
    raw_eval_data = json.load(f)


def create_squad_examples(raw_data):
    squad_examples = []
    for item in raw_data[&quot;data&quot;]:
        for para in item[&quot;paragraphs&quot;]:
            context = para[&quot;context&quot;]
            for qa in para[&quot;qas&quot;]:
                question = qa[&quot;question&quot;]
                answer_text = qa[&quot;answers&quot;][0][&quot;text&quot;]
                all_answers = [_[&quot;text&quot;] for _ in qa[&quot;answers&quot;]]
                start_char_idx = qa[&quot;answers&quot;][0][&quot;answer_start&quot;]
                squad_eg = SquadExample(
                    question, context, start_char_idx, answer_text, all_answers
                )
                squad_eg.preprocess()
                squad_examples.append(squad_eg)
    return squad_examples


def create_inputs_targets(squad_examples):
    dataset_dict = {
        &quot;input_ids&quot;: [],
        &quot;token_type_ids&quot;: [],
        &quot;attention_mask&quot;: [],
        &quot;start_token_idx&quot;: [],
        &quot;end_token_idx&quot;: [],
    }
    for item in squad_examples:
        if item.skip == False:
            for key in dataset_dict:
                dataset_dict[key].append(getattr(item, key))
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict[&quot;input_ids&quot;],
        dataset_dict[&quot;token_type_ids&quot;],
        dataset_dict[&quot;attention_mask&quot;],
    ]
    y = [dataset_dict[&quot;start_token_idx&quot;], dataset_dict[&quot;end_token_idx&quot;]]
    return x, y


train_squad_examples = create_squad_examples(raw_train_data)
x_train, y_train = create_inputs_targets(train_squad_examples)
print(f&quot;{len(train_squad_examples)} training points created.&quot;)

eval_squad_examples = create_squad_examples(raw_eval_data)
x_eval, y_eval = create_inputs_targets(eval_squad_examples)
print(f&quot;{len(eval_squad_examples)} evaluation points created.&quot;)
</code></pre>
<pre><code class="language-bash">87599 training points created.
10570 evaluation points created.
</code></pre>
<p>Create the Question-Answering Model using BERT and Functional API</p>
<pre><code class="language-python">
def create_model():
    ## BERT encoder
    encoder = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)

    ## QA Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)
    embedding = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask
    )[0]

    start_logits = layers.Dense(1, name=&quot;start_logit&quot;, use_bias=False)(embedding)
    start_logits = layers.Flatten()(start_logits)

    end_logits = layers.Dense(1, name=&quot;end_logit&quot;, use_bias=False)(embedding)
    end_logits = layers.Flatten()(end_logits)

    start_probs = layers.Activation(keras.activations.softmax)(start_logits)
    end_probs = layers.Activation(keras.activations.softmax)(end_logits)

    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[start_probs, end_probs],
    )
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=5e-5)
    model.compile(optimizer=optimizer, loss=[loss, loss])
    return model

</code></pre>
<p>This code should preferably be run on Google Colab TPU runtime.
With Colab TPUs, each epoch will take 5-6 minutes.</p>
<pre><code class="language-python">use_tpu = True
if use_tpu:
    # Create distribution strategy
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)

    # Create model
    with strategy.scope():
        model = create_model()
else:
    model = create_model()

model.summary()
</code></pre>
<pre><code class="language-bash">INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0

INFO:tensorflow:Initializing the TPU system: grpc://10.48.159.170:8470

INFO:tensorflow:Clearing out eager caches

INFO:tensorflow:Finished initializing TPU system.

INFO:tensorflow:Found TPU system:

INFO:tensorflow:*** Num TPU Cores: 8

INFO:tensorflow:*** Num TPU Workers: 1

INFO:tensorflow:*** Num TPU Cores Per Worker: 8

Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 384)]        0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 384)]        0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 384)]        0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]                    
__________________________________________________________________________________________________
start_logit (Dense)             (None, 384, 1)       768         tf_bert_model[0][0]              
__________________________________________________________________________________________________
end_logit (Dense)               (None, 384, 1)       768         tf_bert_model[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 384)          0           start_logit[0][0]                
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 384)          0           flatten[0][0]                    
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 384)          0           flatten_1[0][0]                  
==================================================================================================
Total params: 109,483,776
Trainable params: 109,483,776
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
<hr>
<h2>Create evaluation Callback</h2>
<p>This callback will compute the exact match score using the validation data
after every epoch.</p>
<pre><code class="language-python">
def normalize_text(text):
    text = text.lower()

    # Remove punctuations
    exclude = set(string.punctuation)
    text = &quot;&quot;.join(ch for ch in text if ch not in exclude)

    # Remove articles
    regex = re.compile(r&quot;\b(a|an|the)\b&quot;, re.UNICODE)
    text = re.sub(regex, &quot; &quot;, text)

    # Remove extra white space
    text = &quot; &quot;.join(text.split())
    return text


class ExactMatch(keras.callbacks.Callback):
    &quot;&quot;&quot;
    Each `SquadExample` object contains the character level offsets for each token
    in its input paragraph. We use them to get back the span of text corresponding
    to the tokens between our predicted start and end tokens.
    All the ground-truth answers are also present in each `SquadExample` object.
    We calculate the percentage of data points where the span of text obtained
    from model predictions matches one of the ground-truth answers.
    &quot;&quot;&quot;

    def __init__(self, x_eval, y_eval):
        self.x_eval = x_eval
        self.y_eval = y_eval

    def on_epoch_end(self, epoch, logs=None):
        pred_start, pred_end = self.model.predict(self.x_eval)
        count = 0
        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]
        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):
            squad_eg = eval_examples_no_skip[idx]
            offsets = squad_eg.context_token_to_char
            start = np.argmax(start)
            end = np.argmax(end)
            if start &gt;= len(offsets):
                continue
            pred_char_start = offsets[start][0]
            if end &lt; len(offsets):
                pred_char_end = offsets[end][1]
                pred_ans = squad_eg.context[pred_char_start:pred_char_end]
            else:
                pred_ans = squad_eg.context[pred_char_start:]

            normalized_pred_ans = normalize_text(pred_ans)
            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]
            if normalized_pred_ans in normalized_true_ans:
                count += 1
        acc = count / len(self.y_eval[0])
        print(f&quot;\nepoch={epoch+1}, exact match score={acc:.2f}&quot;)

</code></pre>
<hr>
<h2>Train and Evaluate</h2>
<pre><code class="language-python">exact_match_callback = ExactMatch(x_eval, y_eval)
model.fit(
    x_train,
    y_train,
    epochs=1,  # For demonstration, 3 epochs are recommended
    verbose=2,
    batch_size=64,
    callbacks=[exact_match_callback],
)
</code></pre>
<pre><code class="language-bash">
epoch=1, exact match score=0.78
1346/1346 - 350s - activation_7_loss: 1.3488 - loss: 2.5905 - activation_8_loss: 1.2417

&lt;tensorflow.python.keras.callbacks.History at 0x7fc78b4458d0&gt;
</code></pre>
</div></article><div></div></div></main><script src="/_next/static/chunks/webpack-1dd3b446fbaabb6c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/5b284c7119f7d307.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/5f679fa11fe6d7f1.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":3015,\"chunks\":[\"185:static/chunks/app/layout-a78e7a37a1c8ab7c.js\"],\"name\":\"\",\"async\":false}\na:I{\"id\":7767,\"chunks\""])</script><script>self.__next_f.push([1,":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\nb:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5b284c7119f7d307.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"1nTis7GZfnZCbSIdjW90j\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/bert-question-answer/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"bert-question-answer\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"bert-question-answer\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[[\"$\",\"$L9\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-M55V0FS97Q\"}],[\"$\",\"$L9\",null,{\"id\":\"google-analytics\",\"children\":\"\\n              window.dataLayer = window.dataLayer || [];\\n              function gtag(){dataLayer.push(arguments);}\\n              gtag('js', new Date());\\n            \\n              gtag('config', 'G-M55V0FS97Q');\\n            \"}],[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"bert-question-answer\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lc\",\"$Ld\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"bert-question-answer\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5f679fa11fe6d7f1.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"bert-question-answer\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"f:I{\"id\":9699,\"chunks\":[\"12:static/chunks/12-fa30109e9f5a99d5.js\",\"308:static/chunks/app/blog/[slug]/page-fb6b1b72cfb6aeb3.js\"],\"name\":\"Highlight\",\"async\":false}\n7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"BERT (from HuggingFace Transformers) for Text Extraction\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Fine tune pretrained BERT from HuggingFace Transformers on SQuAD.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{"])</script><script>self.__next_f.push([1,"\"name\":\"next-size-adjust\"}]]\nc:null\ne:T3dc5,"])</script><script>self.__next_f.push([1,"\u003cp\u003eCopy of this \u003ca href=\"https://keras.io/examples/nlp/text_extraction_with_bert/\"\u003eexample\u003c/a\u003e I wrote in Keras docs.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis demonstration uses SQuAD (Stanford Question-Answering Dataset).\nIn SQuAD, an input consists of a question, and a paragraph for context.\nThe goal is to find the span of text in the paragraph that answers the question.\nWe evaluate our performance on this data with the \u0026quot;Exact Match\u0026quot; metric,\nwhich measures the percentage of predictions that exactly match any one of the\nground-truth answers.\u003c/p\u003e\n\u003cp\u003eWe fine-tune a BERT model to perform this task as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFeed the context and the question as inputs to BERT.\u003c/li\u003e\n\u003cli\u003eTake two vectors S and T with dimensions equal to that of\nhidden states in BERT.\u003c/li\u003e\n\u003cli\u003eCompute the probability of each token being the start and end of\nthe answer span. The probability of a token being the start of\nthe answer is given by a dot product between S and the representation\nof the token in the last layer of BERT, followed by a softmax over all tokens.\nThe probability of a token being the end of the answer is computed\nsimilarly with the vector T.\u003c/li\u003e\n\u003cli\u003eFine-tune BERT and learn S and T along the way.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eReferences:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1810.04805.pdf\"\u003eBERT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1606.05250\"\u003eSQuAD\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSetup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport os\nimport re\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\nmax_len = 384\nconfiguration = BertConfig()  # default parameters and configuration for BERT\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003eSet-up BERT tokenizer\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;)\nsave_path = \u0026quot;bert_base_uncased/\u0026quot;\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\u0026quot;bert_base_uncased/vocab.txt\u0026quot;, lowercase=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003eLoad the data\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrain_data_url = \u0026quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\u0026quot;\ntrain_path = keras.utils.get_file(\u0026quot;train.json\u0026quot;, train_data_url)\neval_data_url = \u0026quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\u0026quot;\neval_path = keras.utils.get_file(\u0026quot;eval.json\u0026quot;, eval_data_url)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003ePreprocess the data\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eGo through the JSON file and store every record as a \u003ccode\u003eSquadExample\u003c/code\u003e object.\u003c/li\u003e\n\u003cli\u003eGo through each \u003ccode\u003eSquadExample\u003c/code\u003e and create \u003ccode\u003ex_train, y_train, x_eval, y_eval\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nclass SquadExample:\n    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n        self.question = question\n        self.context = context\n        self.start_char_idx = start_char_idx\n        self.answer_text = answer_text\n        self.all_answers = all_answers\n        self.skip = False\n\n    def preprocess(self):\n        context = self.context\n        question = self.question\n        answer_text = self.answer_text\n        start_char_idx = self.start_char_idx\n\n        # Clean context, answer and question\n        context = \u0026quot; \u0026quot;.join(str(context).split())\n        question = \u0026quot; \u0026quot;.join(str(question).split())\n        answer = \u0026quot; \u0026quot;.join(str(answer_text).split())\n\n        # Find end character index of answer in context\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx \u0026gt;= len(context):\n            self.skip = True\n            return\n\n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(context)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Tokenize context\n        tokenized_context = tokenizer.encode(context)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offsets):\n            if sum(is_char_in_ans[start:end]) \u0026gt; 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize question\n        tokenized_question = tokenizer.encode(question)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length \u0026gt; 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length \u0026lt; 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offsets\n\n\nwith open(train_path) as f:\n    raw_train_data = json.load(f)\n\nwith open(eval_path) as f:\n    raw_eval_data = json.load(f)\n\n\ndef create_squad_examples(raw_data):\n    squad_examples = []\n    for item in raw_data[\u0026quot;data\u0026quot;]:\n        for para in item[\u0026quot;paragraphs\u0026quot;]:\n            context = para[\u0026quot;context\u0026quot;]\n            for qa in para[\u0026quot;qas\u0026quot;]:\n                question = qa[\u0026quot;question\u0026quot;]\n                answer_text = qa[\u0026quot;answers\u0026quot;][0][\u0026quot;text\u0026quot;]\n                all_answers = [_[\u0026quot;text\u0026quot;] for _ in qa[\u0026quot;answers\u0026quot;]]\n                start_char_idx = qa[\u0026quot;answers\u0026quot;][0][\u0026quot;answer_start\u0026quot;]\n                squad_eg = SquadExample(\n                    question, context, start_char_idx, answer_text, all_answers\n                )\n                squad_eg.preprocess()\n                squad_examples.append(squad_eg)\n    return squad_examples\n\n\ndef create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \u0026quot;input_ids\u0026quot;: [],\n        \u0026quot;token_type_ids\u0026quot;: [],\n        \u0026quot;attention_mask\u0026quot;: [],\n        \u0026quot;start_token_idx\u0026quot;: [],\n        \u0026quot;end_token_idx\u0026quot;: [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\u0026quot;input_ids\u0026quot;],\n        dataset_dict[\u0026quot;token_type_ids\u0026quot;],\n        dataset_dict[\u0026quot;attention_mask\u0026quot;],\n    ]\n    y = [dataset_dict[\u0026quot;start_token_idx\u0026quot;], dataset_dict[\u0026quot;end_token_idx\u0026quot;]]\n    return x, y\n\n\ntrain_squad_examples = create_squad_examples(raw_train_data)\nx_train, y_train = create_inputs_targets(train_squad_examples)\nprint(f\u0026quot;{len(train_squad_examples)} training points created.\u0026quot;)\n\neval_squad_examples = create_squad_examples(raw_eval_data)\nx_eval, y_eval = create_inputs_targets(eval_squad_examples)\nprint(f\u0026quot;{len(eval_squad_examples)} evaluation points created.\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e87599 training points created.\n10570 evaluation points created.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate the Question-Answering Model using BERT and Functional API\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\ndef create_model():\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\u0026quot;start_logit\u0026quot;, use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\u0026quot;end_logit\u0026quot;, use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code should preferably be run on Google Colab TPU runtime.\nWith Colab TPUs, each epoch will take 5-6 minutes.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003euse_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003eINFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n\nINFO:tensorflow:Initializing the TPU system: grpc://10.48.159.170:8470\n\nINFO:tensorflow:Clearing out eager caches\n\nINFO:tensorflow:Finished initializing TPU system.\n\nINFO:tensorflow:Found TPU system:\n\nINFO:tensorflow:*** Num TPU Cores: 8\n\nINFO:tensorflow:*** Num TPU Workers: 1\n\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\n\nModel: \u0026quot;model\u0026quot;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 384)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]                    \n__________________________________________________________________________________________________\nstart_logit (Dense)             (None, 384, 1)       768         tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nend_logit (Dense)               (None, 384, 1)       768         tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 384)          0           flatten[0][0]                    \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n==================================================================================================\nTotal params: 109,483,776\nTrainable params: 109,483,776\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003eCreate evaluation Callback\u003c/h2\u003e\n\u003cp\u003eThis callback will compute the exact match score using the validation data\nafter every epoch.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\ndef normalize_text(text):\n    text = text.lower()\n\n    # Remove punctuations\n    exclude = set(string.punctuation)\n    text = \u0026quot;\u0026quot;.join(ch for ch in text if ch not in exclude)\n\n    # Remove articles\n    regex = re.compile(r\u0026quot;\\b(a|an|the)\\b\u0026quot;, re.UNICODE)\n    text = re.sub(regex, \u0026quot; \u0026quot;, text)\n\n    # Remove extra white space\n    text = \u0026quot; \u0026quot;.join(text.split())\n    return text\n\n\nclass ExactMatch(keras.callbacks.Callback):\n    \u0026quot;\u0026quot;\u0026quot;\n    Each `SquadExample` object contains the character level offsets for each token\n    in its input paragraph. We use them to get back the span of text corresponding\n    to the tokens between our predicted start and end tokens.\n    All the ground-truth answers are also present in each `SquadExample` object.\n    We calculate the percentage of data points where the span of text obtained\n    from model predictions matches one of the ground-truth answers.\n    \u0026quot;\u0026quot;\u0026quot;\n\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            squad_eg = eval_examples_no_skip[idx]\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start \u0026gt;= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end \u0026lt; len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.context[pred_char_start:]\n\n            normalized_pred_ans = normalize_text(pred_ans)\n            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\u0026quot;\\nepoch={epoch+1}, exact match score={acc:.2f}\u0026quot;)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003eTrain and Evaluate\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eexact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=1,  # For demonstration, 3 epochs are recommended\n    verbose=2,\n    batch_size=64,\n    callbacks=[exact_match_callback],\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e\nepoch=1, exact match score=0.78\n1346/1346 - 350s - activation_7_loss: 1.3488 - loss: 2.5905 - activation_8_loss: 1.2417\n\n\u0026lt;tensorflow.python.keras.callbacks.History at 0x7fc78b4458d0\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"â†– \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"BERT (from HuggingFace Transformers) for Text Extraction\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"23 May 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$e\"}}]]}],[\"$\",\"$Lf\",null,{}]]\n"])</script></body></html>