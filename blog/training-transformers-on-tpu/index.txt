1:HL["/_next/static/media/2aaf0723e720e8b9-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/5b284c7119f7d307.css",{"as":"style"}]
0:["1nTis7GZfnZCbSIdjW90j",[[["",{"children":["blog",{"children":[["slug","training-transformers-on-tpu","d"],{"children":["__PAGE__?{\"slug\":\"training-transformers-on-tpu\"}",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5b284c7119f7d307.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/media/b89f66ecdb077e7f-s.p.woff2",{"as":"font","type":"font/woff2"}]
6:HL["/_next/static/css/5f679fa11fe6d7f1.css",{"as":"style"}]
7:I{"id":3015,"chunks":["185:static/chunks/app/layout-a78e7a37a1c8ab7c.js"],"name":"","async":false}
8:I{"id":7767,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
9:I{"id":7920,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
3:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"className":"bg-[#1a1a1a] text-neutral-200 __className_20951f","children":["$","main",null,{"className":"flex min-h-screen flex-col items-center py-16 md:py-32 px-6","children":[["$","$L7",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-M55V0FS97Q"}],["$","$L7",null,{"id":"google-analytics","children":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n            \n              gtag('config', 'G-M55V0FS97Q');\n            "}],["$","div",null,{"className":"max-w-lg w-full flex flex-col","children":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children",["slug","training-transformers-on-tpu","d"],"children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$La","$Lb",null],"segment":"__PAGE__?{\"slug\":\"training-transformers-on-tpu\"}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5f679fa11fe6d7f1.css","precedence":"next"}]]}],"segment":["slug","training-transformers-on-tpu","d"]},"styles":[]}],"segment":"blog"},"styles":[]}]}]]}]}]}],null]
d:I{"id":9699,"chunks":["12:static/chunks/12-fa30109e9f5a99d5.js","308:static/chunks/app/blog/[slug]/page-fb6b1b72cfb6aeb3.js"],"name":"Highlight","async":false}
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Training Transformers on TPU"}],["$","meta","2",{"name":"description","content":"Training transformers is now super easy thanks to HuggingFace and Tensorflow 2"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","4",{"name":"next-size-adjust"}]]
a:null
c:Te77,<p>This post shows how clean and straightforward it is to use huggingface library and tensorflow 2 to train transformer models on GPU and TPU!</p>
<p>As of today (April, 2020),
this is how you fine tune a BERT model on GPU (Same code works for CPU as well).</p>
<pre><code class="language-python"># load data
#
# df = load_data()
# -----------------------
# text            , label
# -----------------------
# good job.           1
# oh no!              0
# .....

# tokenize text
text_col = df.text.astype(str)
input_ids = []
for i in range(0, len(text_col), config.chunk_size):
    text_chunk = text_col[i:i+config.chunk_size].tolist()
    encoded = tokenizer.encode_batch(text_chunk)
    input_ids.extend([enc.ids for enc in encoded])

x_train = np.array(input_ids)
y_train = df.label.values

# create data loader
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_train, y_train))
    .repeat()
    .shuffle(2048)
    .batch(config.batch_size)
    .prefetch(tf.data.experimental.AUTOTUNE)
)

# create model : Input &gt; BERT &gt; Dense
bert = TFBertModel.from_pretrained(config.MODEL_PATH, config=BertConfig())
input_ids = layers.Input(shape=(config.maxlen,), dtype=tf.int32)
sequence_output = bert(input_ids)[0][:, 0, :]
out = layers.Dense(1, activation=&#39;sigmoid&#39;)(sequence_output)
classifier = models.Model(inputs=input_ids, outputs=out)

# compile
classifier.compile(optimizers.Adam(lr=3e-5), 
                   loss=&#39;binary_crossentropy&#39;, 
                   metrics=[metrics.AUC()])

# train !
train_history = classifier.fit(
    train_dataset,
    steps_per_epoch=150,
    epochs=30
)
</code></pre>
<p>And this is how you do the same thing on TPUs. (Few extra lines added to load the model on TPU)</p>
<pre><code class="language-python"># load data
#
# df = load_data()
# -----------------------
# text            , label
# -----------------------
# good job.           1
# oh no!              0
# .....

# create distribution strategy
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu)

# create model : Input &gt; BERT &gt; Dense
with strategy.scope():
    bert = TFBertModel.from_pretrained(config.MODEL_PATH, config=BertConfig())
    input_ids = layers.Input(shape=(config.maxlen,), dtype=tf.int32)
    sequence_output = bert(input_ids)[0][:, 0, :]
    out = layers.Dense(1, activation=&#39;sigmoid&#39;)(sequence_output)
    classifier = models.Model(inputs=input_ids, outputs=out)
    
    classifier.compile(optimizers.Adam(lr=3e-5), 
                   loss=&#39;binary_crossentropy&#39;, 
                   metrics=[metrics.AUC()])
    
# tokenize text
text_col = df.text.astype(str)
input_ids = []
for i in tqdm(range(0, len(text_col), config.chunk_size)):
    text_chunk = text_col[i:i+config.chunk_size].tolist()
    encoded = tokenizer.encode_batch(text_chunk)
    input_ids.extend([enc.ids for enc in encoded])

x_train = np.array(input_ids)
y_train = df.label.values

# create data loader
train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((x_train, y_train))
    .repeat()
    .shuffle(2048)
    .batch(config.batch_size)
    .prefetch(tf.data.experimental.AUTOTUNE)
)

# train !
train_history = classifier.fit(
    train_dataset,
    steps_per_epoch=150,
    epochs=30
)
</code></pre>
<p>Its honestly amazing how easy this is compared to the old tensorflow 1.1x flow in the original code base of BERT paper!</p>
<p>This abstraction and creation of clean APIs for latest deep learning techniques and models will continue to lower the barrier for using them in applications.</p>
b:[["$","div",null,{"className":"mb-7 font-light","children":["â†– ",["$","a",null,{"rel":"","target":"","className":" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400","href":"/blog","children":"Back"}]]}],["$","article",null,{"className":"__className_bfc3c1 leading-7 antialiased","children":[["$","h1",null,{"children":"Training Transformers on TPU"}],["$","p",null,{"className":"text-neutral-500","children":"19 April 2020"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$c"}}]]}],["$","$Ld",null,{}]]
