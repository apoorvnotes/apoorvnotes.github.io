1:HL["/_next/static/media/2aaf0723e720e8b9-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/5b284c7119f7d307.css",{"as":"style"}]
0:["L48R4dzp_TPIykvQfQNeq",[[["",{"children":["blog",{"children":[["slug","bert-pretraining","d"],{"children":["__PAGE__?{\"slug\":\"bert-pretraining\"}",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5b284c7119f7d307.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/media/b89f66ecdb077e7f-s.p.woff2",{"as":"font","type":"font/woff2"}]
6:HL["/_next/static/css/5f679fa11fe6d7f1.css",{"as":"style"}]
7:I{"id":7767,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
8:I{"id":7920,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
3:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"className":"bg-[#1a1a1a] text-neutral-200 __className_20951f","children":["$","main",null,{"className":"flex min-h-screen flex-col items-center py-16 md:py-32 px-6","children":["$","div",null,{"className":"max-w-lg w-full flex flex-col","children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children",["slug","bert-pretraining","d"],"children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L9","$La",null],"segment":"__PAGE__?{\"slug\":\"bert-pretraining\"}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5f679fa11fe6d7f1.css","precedence":"next"}]]}],"segment":["slug","bert-pretraining","d"]},"styles":[]}],"segment":"blog"},"styles":[]}]}]}]}]}],null]
c:I{"id":9699,"chunks":["12:static/chunks/12-fa30109e9f5a99d5.js","308:static/chunks/app/blog/[slug]/page-fb6b1b72cfb6aeb3.js"],"name":"Highlight","async":false}
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"BERT masked LM training"}],["$","meta","2",{"name":"description","content":"Pretraining or fine tuning BERT on masked LM task"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","4",{"name":"next-size-adjust"}]]
9:null
b:T16cb,<h2>Initial Setup</h2>
<p>I will use BERT model from huggingface and a lighweight wrapper over pytorch
called <a href="https://github.com/PyTorchLightning/pytorch-lightning">Pytorch Lightning</a> to avoid writing boilerplate.<br/></p>
<pre><code class="language-bash">!pip install transformers
!pip install pytorch-lightning
</code></pre>
<p>To run this over TPUs, the following dependencies are also needed.<br/></p>
<pre><code class="language-bash">!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev
</code></pre>
<p>To demonstrate I&#39;ll use a text corpus, which can be downloaded as follows:<br/></p>
<pre><code class="language-python">
import urllib.request
txt_url = &quot;https://s3.amazonaws.com/text-datasets/nietzsche.txt&quot;
urllib.request.urlretrieve(txt_url, &#39;train.txt&#39;)
</code></pre>
<h2>Imports and Configs<br/></h2>
<pre><code class="language-python">
import pytorch_lightning as pl
from argparse import Namespace
from transformers import (
    BertConfig,
    BertForMaskedLM,
    BertTokenizer,
    AdamW,
    DataCollatorForLanguageModeling
)
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

args = Namespace()
args.train = &quot;train.txt&quot;
args.max_len = 128
args.model_name = &quot;bert-base-uncased&quot;
args.epochs = 1
args.batch_size = 4
</code></pre>
<h2>Create Dataloader<br/></h2>
<p>The Dataset class reads a text file.
Each line in the file forms a single element of the dataset after tokenization with BERT&#39;s tokenizer.</p>
<pre><code class="language-python">
tokenizer = BertTokenizer.from_pretrained(args.model_name)

class MaskedLMDataset(Dataset):
    def __init__(self, file, tokenizer):
        self.tokenizer = tokenizer
        self.lines = self.load_lines(file)
        self.ids = self.encode_lines(self.lines)
        
    def load_lines(self, file):
        with open(file) as f:
            lines = [
                line
                for line in f.read().splitlines()
                if (len(line) &gt; 0 and not line.isspace())
            ]
        return lines
    
    def encode_lines(self, lines):
        batch_encoding = self.tokenizer(
            lines, add_special_tokens=True, truncation=True, max_length=args.max_len
        )
        return batch_encoding[&quot;input_ids&quot;]

    def __len__(self):
        return len(self.lines)

    def __getitem__(self, idx):
        return torch.tensor(self.ids[idx], dtype=torch.long)
        
train_dataset = MaskedLMDataset(args.train, tokenizer)
</code></pre>
<p>A collator function in pytorch takes a list of elements given by the dataset class and
and creates a batch of input (and targets). Huggingface provides a convenient collator function
which takes a list of input ids from my dataset, masks 15% of the tokens, and
creates a batch after appropriate padding.</p>
<p>Targets are created by cloning the input ids. Then, if a token is supposed to be masked, the corresponding input id is replaced 
by that of either the [MASK] token (80% chance), a random token (10% chance), the same token (10% chance).
If a token is not supposed to be masked, the corresponding target id is replaced by -100, so that they are ignored during loss calculation.</p>
<pre><code class="language-python">data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

train_loader = DataLoader(
    train_dataset,
    batch_size=args.bs,
    collate_fn=data_collator
)
</code></pre>
<h2>Define model, training step and optmizer<br/></h2>
<pre><code class="language-python">class Bert(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.bert = BertForMaskedLM.from_pretrained(args.model_name)

    def forward(self, input_ids, labels):
        return self.bert(input_ids=input_ids,labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;]
        labels = batch[&quot;labels&quot;]
        outputs = self(input_ids=input_ids, labels=labels)
        loss = outputs[0]
        return {&quot;loss&quot;: loss}

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr=1e-5)

model = Bert()
</code></pre>
<h2>Train<br/></h2>
<p>This is where pytorch lightning does an awesome job. Once the model and
data loader are ready, I can train on CPU, single GPU, multiple GPUs, single TPU core and multiple TPU cores with just two lines of code.<br></p>
<ol>
<li>Initialise the Trainer as per the hardware:<br>
 CPU<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1)
</code></pre>
 GPU (single or multiple)<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, gpus=8)
</code></pre>
 Single TPU core<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, tpu_cores=[1])
</code></pre>
 Multiple TPU cores<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, tpu_cores=8)
</code></pre>
</li>
<li>Run the fit function.</li>
</ol>
<pre><code class="language-python">trainer = pl.Trainer(gpus=1)
trainer.fit(model, train_loader)
</code></pre>
<h2>Saving and Loading</h2>
<p>The weights can be saved and loaded for predictions like this.<br></p>
<pre><code class="language-python">torch.save(model.state_dict(), &#39;saved.bin&#39;)

class BertPred(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)

    def forward(self, input_ids, labels=None):
        return self.bert(input_ids=input_ids,labels=labels)

new_model = BERTPred()
new_model.load_state_dict(torch.load(&#39;saved.bin&#39;))
new_model.eval()
</code></pre>
a:[["$","div",null,{"className":"mb-7 font-light","children":["↖ ",["$","a",null,{"rel":"","target":"","className":" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400","href":"/blog","children":"Back"}]]}],["$","article",null,{"className":"__className_bfc3c1 leading-7 antialiased","children":[["$","h1",null,{"children":"BERT masked LM training"}],["$","p",null,{"className":"text-neutral-500","children":"15 August 2020"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]]}],["$","$Lc",null,{}]]
