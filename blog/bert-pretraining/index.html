<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/5b284c7119f7d307.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5f679fa11fe6d7f1.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-1dd3b446fbaabb6c.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-44fe9588d0e5e7bc.js" async=""></script><script src="/_next/static/chunks/596-27146c691e1092ff.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=G-M55V0FS97Q"/><title>BERT masked LM training</title><meta name="description" content="Pretraining or fine tuning BERT on masked LM task"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">↖ <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>BERT masked LM training</h1><p class="text-neutral-500">15 August 2020</p><div><h2>Initial Setup</h2>
<p>I will use BERT model from huggingface and a lighweight wrapper over pytorch
called <a href="https://github.com/PyTorchLightning/pytorch-lightning">Pytorch Lightning</a> to avoid writing boilerplate.<br/></p>
<pre><code class="language-bash">!pip install transformers
!pip install pytorch-lightning
</code></pre>
<p>To run this over TPUs, the following dependencies are also needed.<br/></p>
<pre><code class="language-bash">!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev
</code></pre>
<p>To demonstrate I&#39;ll use a text corpus, which can be downloaded as follows:<br/></p>
<pre><code class="language-python">
import urllib.request
txt_url = &quot;https://s3.amazonaws.com/text-datasets/nietzsche.txt&quot;
urllib.request.urlretrieve(txt_url, &#39;train.txt&#39;)
</code></pre>
<h2>Imports and Configs<br/></h2>
<pre><code class="language-python">
import pytorch_lightning as pl
from argparse import Namespace
from transformers import (
    BertConfig,
    BertForMaskedLM,
    BertTokenizer,
    AdamW,
    DataCollatorForLanguageModeling
)
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

args = Namespace()
args.train = &quot;train.txt&quot;
args.max_len = 128
args.model_name = &quot;bert-base-uncased&quot;
args.epochs = 1
args.batch_size = 4
</code></pre>
<h2>Create Dataloader<br/></h2>
<p>The Dataset class reads a text file.
Each line in the file forms a single element of the dataset after tokenization with BERT&#39;s tokenizer.</p>
<pre><code class="language-python">
tokenizer = BertTokenizer.from_pretrained(args.model_name)

class MaskedLMDataset(Dataset):
    def __init__(self, file, tokenizer):
        self.tokenizer = tokenizer
        self.lines = self.load_lines(file)
        self.ids = self.encode_lines(self.lines)
        
    def load_lines(self, file):
        with open(file) as f:
            lines = [
                line
                for line in f.read().splitlines()
                if (len(line) &gt; 0 and not line.isspace())
            ]
        return lines
    
    def encode_lines(self, lines):
        batch_encoding = self.tokenizer(
            lines, add_special_tokens=True, truncation=True, max_length=args.max_len
        )
        return batch_encoding[&quot;input_ids&quot;]

    def __len__(self):
        return len(self.lines)

    def __getitem__(self, idx):
        return torch.tensor(self.ids[idx], dtype=torch.long)
        
train_dataset = MaskedLMDataset(args.train, tokenizer)
</code></pre>
<p>A collator function in pytorch takes a list of elements given by the dataset class and
and creates a batch of input (and targets). Huggingface provides a convenient collator function
which takes a list of input ids from my dataset, masks 15% of the tokens, and
creates a batch after appropriate padding.</p>
<p>Targets are created by cloning the input ids. Then, if a token is supposed to be masked, the corresponding input id is replaced 
by that of either the [MASK] token (80% chance), a random token (10% chance), the same token (10% chance).
If a token is not supposed to be masked, the corresponding target id is replaced by -100, so that they are ignored during loss calculation.</p>
<pre><code class="language-python">data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

train_loader = DataLoader(
    train_dataset,
    batch_size=args.bs,
    collate_fn=data_collator
)
</code></pre>
<h2>Define model, training step and optmizer<br/></h2>
<pre><code class="language-python">class Bert(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.bert = BertForMaskedLM.from_pretrained(args.model_name)

    def forward(self, input_ids, labels):
        return self.bert(input_ids=input_ids,labels=labels)

    def training_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;]
        labels = batch[&quot;labels&quot;]
        outputs = self(input_ids=input_ids, labels=labels)
        loss = outputs[0]
        return {&quot;loss&quot;: loss}

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr=1e-5)

model = Bert()
</code></pre>
<h2>Train<br/></h2>
<p>This is where pytorch lightning does an awesome job. Once the model and
data loader are ready, I can train on CPU, single GPU, multiple GPUs, single TPU core and multiple TPU cores with just two lines of code.<br></p>
<ol>
<li>Initialise the Trainer as per the hardware:<br>
 CPU<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1)
</code></pre>
 GPU (single or multiple)<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, gpus=8)
</code></pre>
 Single TPU core<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, tpu_cores=[1])
</code></pre>
 Multiple TPU cores<br><pre><code class="language-python">trainer = pl.Trainer(max_epochs=1, tpu_cores=8)
</code></pre>
</li>
<li>Run the fit function.</li>
</ol>
<pre><code class="language-python">trainer = pl.Trainer(gpus=1)
trainer.fit(model, train_loader)
</code></pre>
<h2>Saving and Loading</h2>
<p>The weights can be saved and loaded for predictions like this.<br></p>
<pre><code class="language-python">torch.save(model.state_dict(), &#39;saved.bin&#39;)

class BertPred(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)

    def forward(self, input_ids, labels=None):
        return self.bert(input_ids=input_ids,labels=labels)

new_model = BERTPred()
new_model.load_state_dict(torch.load(&#39;saved.bin&#39;))
new_model.eval()
</code></pre>
</div></article><div></div></div></main><script src="/_next/static/chunks/webpack-1dd3b446fbaabb6c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/5b284c7119f7d307.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/5f679fa11fe6d7f1.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":3015,\"chunks\":[\"185:static/chunks/app/layout-a78e7a37a1c8ab7c.js\"],\"name\":\"\",\"async\":false}\na:I{\"id\":7767,\"chunks\""])</script><script>self.__next_f.push([1,":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\nb:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-1dd3b446fbaabb6c.js\",\"971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js\",\"596:static/chunks/596-27146c691e1092ff.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5b284c7119f7d307.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"1nTis7GZfnZCbSIdjW90j\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/bert-pretraining/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"bert-pretraining\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"bert-pretraining\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[[\"$\",\"$L9\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-M55V0FS97Q\"}],[\"$\",\"$L9\",null,{\"id\":\"google-analytics\",\"children\":\"\\n              window.dataLayer = window.dataLayer || [];\\n              function gtag(){dataLayer.push(arguments);}\\n              gtag('js', new Date());\\n            \\n              gtag('config', 'G-M55V0FS97Q');\\n            \"}],[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"bert-pretraining\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lc\",\"$Ld\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"bert-pretraining\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5f679fa11fe6d7f1.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"bert-pretraining\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"BERT masked LM training\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Pretraining or fine tuning BERT on masked LM task\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"f:I{\"id\":9699,\"chunks\":[\"12:static/chunks/12-fa30109e9f5a99d5.js\",\"308:static/chunks/app/blog/[slug]/page-fb6b1b72cfb6aeb3.js\"],\"name\":\"Highlight\",\"async\":false}\ne:T16cb,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eInitial Setup\u003c/h2\u003e\n\u003cp\u003eI will use BERT model from huggingface and a lighweight wrapper over pytorch\ncalled \u003ca href=\"https://github.com/PyTorchLightning/pytorch-lightning\"\u003ePytorch Lightning\u003c/a\u003e to avoid writing boilerplate.\u003cbr/\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e!pip install transformers\n!pip install pytorch-lightning\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run this over TPUs, the following dependencies are also needed.\u003cbr/\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo demonstrate I\u0026#39;ll use a text corpus, which can be downloaded as follows:\u003cbr/\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nimport urllib.request\ntxt_url = \u0026quot;https://s3.amazonaws.com/text-datasets/nietzsche.txt\u0026quot;\nurllib.request.urlretrieve(txt_url, \u0026#39;train.txt\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eImports and Configs\u003cbr/\u003e\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\nimport pytorch_lightning as pl\nfrom argparse import Namespace\nfrom transformers import (\n    BertConfig,\n    BertForMaskedLM,\n    BertTokenizer,\n    AdamW,\n    DataCollatorForLanguageModeling\n)\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nargs = Namespace()\nargs.train = \u0026quot;train.txt\u0026quot;\nargs.max_len = 128\nargs.model_name = \u0026quot;bert-base-uncased\u0026quot;\nargs.epochs = 1\nargs.batch_size = 4\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreate Dataloader\u003cbr/\u003e\u003c/h2\u003e\n\u003cp\u003eThe Dataset class reads a text file.\nEach line in the file forms a single element of the dataset after tokenization with BERT\u0026#39;s tokenizer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\ntokenizer = BertTokenizer.from_pretrained(args.model_name)\n\nclass MaskedLMDataset(Dataset):\n    def __init__(self, file, tokenizer):\n        self.tokenizer = tokenizer\n        self.lines = self.load_lines(file)\n        self.ids = self.encode_lines(self.lines)\n        \n    def load_lines(self, file):\n        with open(file) as f:\n            lines = [\n                line\n                for line in f.read().splitlines()\n                if (len(line) \u0026gt; 0 and not line.isspace())\n            ]\n        return lines\n    \n    def encode_lines(self, lines):\n        batch_encoding = self.tokenizer(\n            lines, add_special_tokens=True, truncation=True, max_length=args.max_len\n        )\n        return batch_encoding[\u0026quot;input_ids\u0026quot;]\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.ids[idx], dtype=torch.long)\n        \ntrain_dataset = MaskedLMDataset(args.train, tokenizer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA collator function in pytorch takes a list of elements given by the dataset class and\nand creates a batch of input (and targets). Huggingface provides a convenient collator function\nwhich takes a list of input ids from my dataset, masks 15% of the tokens, and\ncreates a batch after appropriate padding.\u003c/p\u003e\n\u003cp\u003eTargets are created by cloning the input ids. Then, if a token is supposed to be masked, the corresponding input id is replaced \nby that of either the [MASK] token (80% chance), a random token (10% chance), the same token (10% chance).\nIf a token is not supposed to be masked, the corresponding target id is replaced by -100, so that they are ignored during loss calculation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=args.bs,\n    collate_fn=data_collator\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDefine model, training step and optmizer\u003cbr/\u003e\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass Bert(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.bert = BertForMaskedLM.from_pretrained(args.model_name)\n\n    def forward(self, input_ids, labels):\n        return self.bert(input_ids=input_ids,labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\u0026quot;input_ids\u0026quot;]\n        labels = batch[\u0026quot;labels\u0026quot;]\n        outputs = self(input_ids=input_ids, labels=labels)\n        loss = outputs[0]\n        return {\u0026quot;loss\u0026quot;: loss}\n\n    def configure_optimizers(self):\n        return AdamW(self.parameters(), lr=1e-5)\n\nmodel = Bert()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTrain\u003cbr/\u003e\u003c/h2\u003e\n\u003cp\u003eThis is where pytorch lightning does an awesome job. Once the model and\ndata loader are ready, I can train on CPU, single GPU, multiple GPUs, single TPU core and multiple TPU cores with just two lines of code.\u003cbr\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInitialise the Trainer as per the hardware:\u003cbr\u003e\n CPU\u003cbr\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrainer = pl.Trainer(max_epochs=1)\n\u003c/code\u003e\u003c/pre\u003e\n GPU (single or multiple)\u003cbr\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrainer = pl.Trainer(max_epochs=1, gpus=8)\n\u003c/code\u003e\u003c/pre\u003e\n Single TPU core\u003cbr\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrainer = pl.Trainer(max_epochs=1, tpu_cores=[1])\n\u003c/code\u003e\u003c/pre\u003e\n Multiple TPU cores\u003cbr\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrainer = pl.Trainer(max_epochs=1, tpu_cores=8)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eRun the fit function.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etrainer = pl.Trainer(gpus=1)\ntrainer.fit(model, train_loader)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSaving and Loading\u003c/h2\u003e\n\u003cp\u003eThe weights can be saved and loaded for predictions like this.\u003cbr\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etorch.save(model.state_dict(), \u0026#39;saved.bin\u0026#39;)\n\nclass BertPred(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertForMaskedLM.from_pretrained(\u0026#39;bert-base-uncased\u0026#39;)\n\n    def forward(self, input_ids, labels=None):\n        return self.bert(input_ids=input_ids,labels=labels)\n\nnew_model = BERTPred()\nnew_model.load_state_dict(torch.load(\u0026#39;saved.bin\u0026#39;))\nnew_model.eval()\n\u003c/code\u003e\u003c/pre\u003e\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"↖ \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"BERT masked LM training\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"15 August 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$e\"}}]]}],[\"$\",\"$Lf\",null,{}]]\n"])</script></body></html>