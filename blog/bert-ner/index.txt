1:HL["/_next/static/media/2aaf0723e720e8b9-s.p.woff2",{"as":"font","type":"font/woff2"}]
2:HL["/_next/static/css/5b284c7119f7d307.css",{"as":"style"}]
0:["L48R4dzp_TPIykvQfQNeq",[[["",{"children":["blog",{"children":[["slug","bert-ner","d"],{"children":["__PAGE__?{\"slug\":\"bert-ner\"}",{}]}]}]},"$undefined","$undefined",true],"$L3",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5b284c7119f7d307.css","precedence":"next"}]],"$L4"]]]]
5:HL["/_next/static/media/b89f66ecdb077e7f-s.p.woff2",{"as":"font","type":"font/woff2"}]
6:HL["/_next/static/css/5f679fa11fe6d7f1.css",{"as":"style"}]
7:I{"id":7767,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
8:I{"id":7920,"chunks":["272:static/chunks/webpack-1dd3b446fbaabb6c.js","971:static/chunks/fd9d1056-44fe9588d0e5e7bc.js","596:static/chunks/596-27146c691e1092ff.js"],"name":"default","async":false}
3:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"className":"bg-[#1a1a1a] text-neutral-200 __className_20951f","children":["$","main",null,{"className":"flex min-h-screen flex-col items-center py-16 md:py-32 px-6","children":["$","div",null,{"className":"max-w-lg w-full flex flex-col","children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children",["slug","bert-ner","d"],"children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$L9","$La",null],"segment":"__PAGE__?{\"slug\":\"bert-ner\"}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5f679fa11fe6d7f1.css","precedence":"next"}]]}],"segment":["slug","bert-ner","d"]},"styles":[]}],"segment":"blog"},"styles":[]}]}]}]}]}],null]
4:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Entity Recognition with BERT"}],["$","meta","2",{"name":"description","content":"Using tf.keras and huggingface for NER"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","4",{"name":"next-size-adjust"}]]
9:null
c:I{"id":9699,"chunks":["12:static/chunks/12-fa30109e9f5a99d5.js","308:static/chunks/app/blog/[slug]/page-fb6b1b72cfb6aeb3.js"],"name":"Highlight","async":false}
b:T2d3f,<h2>Introduction</h2>
<p>This post uses BERT (from huggingface) and tf.keras to train a NER model.
The data is expected in this format:</p>
<pre><code>    Sentence #	 Word            Tag
0   Sentence: 1	 Thousands       O
1   NaN	         of   	        O
2   NaN	         demonstrators  O
3   NaN	         have           O
4   NaN	         marched        O
5   NaN	         through        O
6   NaN	         London         B-geo
7   NaN	         to             O
8   NaN	         protest        O
...
...
24  Sentence: 2	 Families       O
25  NaN	         of             O
26  NaN	         soldiers       O
...
</code></pre>
<h2>Setup</h2>
<pre><code class="language-python">import os
import re
import json
import string
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn import preprocessing
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig

max_len = 384
configuration = BertConfig()
data_csv = &quot;ner_dataset.csv&quot;
</code></pre>
<h2>Setup Tokenizers</h2>
<pre><code class="language-python"># Save the slow pretrained tokenizer
slow_tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
save_path = &quot;bert_base_uncased/&quot;
if not os.path.exists(save_path):
    os.makedirs(save_path)
slow_tokenizer.save_pretrained(save_path)

# Load the fast tokenizer from saved file
tokenizer = BertWordPieceTokenizer(&quot;bert_base_uncased/vocab.txt&quot;, lowercase=True)
</code></pre>
<h2>Define model</h2>
<p><strong>Model</strong><br>Add a fully connected layer that takes token embeddings from BERT as input and<br>predicts probability of that token belonging to each of the possible tags.<br>The fully connected layer in the code below shows a <code>keras.layers.Dense</code> layer<br>with <code>num_tags+1</code> units to accomodate a padding label.</p>
<p><strong>Masked Loss</strong><br>Each batch of data will consist of variable sized sentence tokens with<br>appropriate padding in both input and target.<br>During loss calculation, we ignore the loss corresponding padding tokens<br>in the target.</p>
<pre><code class="language-python">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False, reduction=tf.keras.losses.Reduction.NONE
)

def masked_ce_loss(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 17))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)

def create_model(num_tags):
    ## BERT encoder
    encoder = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)

    ## NER Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)
    embedding = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask
    )[0]
    embedding = layers.Dropout(0.3)(embedding)
    tag_logits = layers.Dense(num_tags+1, activation=&#39;softmax&#39;)(embedding)
    
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[tag_logits],
    )
    optimizer = keras.optimizers.Adam(lr=3e-5)
    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=[&#39;accuracy&#39;])
    return model
</code></pre>
<h2>Preprocess dataset</h2>
<p>First, we read the convert the rows of our data file into sentences and lists of<br>tags. <code>sklearn.preprocessing.LabelEncoder</code> encodes each tag in a number.<br>Then, we create tokenize each sentence using BERT tokenizer from huggingface.<br>After tokenization each sentence is represented by a set of input_ids,<br>attention_masks and token_type_ids. Note that a single word may be tokenized into<br>multiple tokens. In that case, each token gets the label of the original word.<br>eg.</p>
<pre><code>this is   a    differentiator -&gt; [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;different&#39;, &#39;##ia&#39;, &#39;##tor&#39;]
tag1 tag2 tag3 tag4           -&gt; [tag1,   tag2, tag3, tag4,       tag4,   tag4]
</code></pre>
<pre><code class="language-python">def process_csv(data_path):
    df = pd.read_csv(data_path, encoding=&quot;latin-1&quot;)
    df.loc[:, &quot;Sentence #&quot;] = df[&quot;Sentence #&quot;].fillna(method=&quot;ffill&quot;)
    enc_tag = preprocessing.LabelEncoder()
    df.loc[:, &quot;Tag&quot;] = enc_tag.fit_transform(df[&quot;Tag&quot;])
    sentences = df.groupby(&quot;Sentence #&quot;)[&quot;Word&quot;].apply(list).values
    tag = df.groupby(&quot;Sentence #&quot;)[&quot;Tag&quot;].apply(list).values
    return sentences, tag, enc_tag


def create_inputs_targets(data_csv):
    dataset_dict = {
        &quot;input_ids&quot;: [],
        &quot;token_type_ids&quot;: [],
        &quot;attention_mask&quot;: [],
        &quot;tags&quot;: []
    }
    sentences, tags, tag_encoder = process_csv(data_csv)
    
    for sentence, tag in zip(sentences, tags):
        input_ids = []
        target_tags = []
        for idx, word in enumerate(sentence):
            ids = tokenizer.encode(word, add_special_tokens=False)
            input_ids.extend(ids.ids)
            num_tokens = len(ids)
            target_tags.extend([tag[idx]] * num_tokens)
        
        
        # Pad truncate
        input_ids = input_ids[:max_len - 2]
        target_tags = target_tags[:max_len - 2]

        input_ids = [101] + input_ids + [102]
        target_tags = [16] + target_tags + [16]
        token_type_ids = [0] * len(input_ids)
        attention_mask = [1] * len(input_ids)
        padding_len = max_len - len(input_ids)

        input_ids = input_ids + ([0] * padding_len)
        attention_mask = attention_mask + ([0] * padding_len)
        token_type_ids = token_type_ids + ([0] * padding_len)
        target_tags = target_tags + ([17] * padding_len)
        
        dataset_dict[&quot;input_ids&quot;].append(input_ids)
        dataset_dict[&quot;token_type_ids&quot;].append(token_type_ids)
        dataset_dict[&quot;attention_mask&quot;].append(attention_mask)
        dataset_dict[&quot;tags&quot;].append(target_tags)
        assert len(target_tags) == max_len, f&#39;{len(input_ids)}, {len(target_tags)}&#39;
        
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict[&quot;input_ids&quot;],
        dataset_dict[&quot;token_type_ids&quot;],
        dataset_dict[&quot;attention_mask&quot;],
    ]
    y = dataset_dict[&quot;tags&quot;]
    return x, y, tag_encoder
</code></pre>
<h2>Create model</h2>
<p>Use TPU if possible.</p>
<pre><code class="language-python">num_tags = pd.read_csv(data_csv, encoding=&quot;latin-1&quot;)[&quot;Tag&quot;].nunique()

use_tpu = None
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    use_tpu = True
except:
    use_tpu = False

if use_tpu:
    # Create distribution strategy
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)

    # Create model
    with strategy.scope():
        model = create_model(num_tags)
else:
    model = create_model(num_tags)
    
model.summary()
</code></pre>
<h1>Output:
```
Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     </h1>
<h1>input_1 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]<br>__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 384, 768)     0           tf_bert_model[0][0]<br>__________________________________________________________________________________________________
dense (Dense)                   (None, 384, 18)      13842       dropout_37[0][0]                 </h1>
<p>Total params: 109,496,082
Trainable params: 109,496,082
Non-trainable params: 0</p>
<hr>
<pre><code>
## Train

```python
x_train, y_train, tag_encoder = create_inputs_targets(data_csv)

bs = 64 if use_tpu else 16

model.fit(
    x_train,
    y_train,
    epochs=1,
    verbose=1,
    batch_size=bs,
    validation_split=0.1
)
</code></pre>
<p>Output:</p>
<pre><code>675/675 [==============================] - 151s 223ms/step - accuracy: 0.9909 - loss: nan - val_accuracy: 0.9969 - val_loss: 2.3592
</code></pre>
<h2>Inference</h2>
<p>Check the predicted tags for a sample sentence.</p>
<pre><code class="language-python">def create_test_input_from_text(texts):
    dataset_dict = {
        &quot;input_ids&quot;: [],
        &quot;token_type_ids&quot;: [],
        &quot;attention_mask&quot;: []
    }
    for sentence in texts:
        input_ids = []
        for idx, word in enumerate(sentence.split()):
            ids = tokenizer.encode(word, add_special_tokens=False)
            input_ids.extend(ids.ids)
            num_tokens = len(ids)
            
        # Pad and create attention masks.
        # Skip if truncation is needed
        input_ids = input_ids[:max_len - 2]

        input_ids = [101] + input_ids + [102]
        n_tokens = len(input_ids)
        token_type_ids = [0] * len(input_ids)
        attention_mask = [1] * len(input_ids)
        padding_len = max_len - len(input_ids)

        input_ids = input_ids + ([0] * padding_len)
        attention_mask = attention_mask + ([0] * padding_len)
        token_type_ids = token_type_ids + ([0] * padding_len)
        
        dataset_dict[&quot;input_ids&quot;].append(input_ids)
        dataset_dict[&quot;token_type_ids&quot;].append(token_type_ids)
        dataset_dict[&quot;attention_mask&quot;].append(attention_mask)
        
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict[&quot;input_ids&quot;],
        dataset_dict[&quot;token_type_ids&quot;],
        dataset_dict[&quot;attention_mask&quot;],
    ]
    return x, n_tokens

test_inputs = [&quot;alex lives in london&quot;]
x_test, n_tokens = create_test_input_from_text(test_inputs)
print(&#39;input tokens&#39;)
print(x_test[0][0][:n_tokens])
pred_test = model.predict(x_test)
pred_tags = np.argmax(pred_test,2)[0][:n_tokens]  # ignore predictions of padding tokens

# create dictionary of tags and and their indexes
le_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))
print(&#39;predicted tags&#39;)
print([le_dict.get(_, &#39;[pad]&#39;) for _ in pred_tags])
</code></pre>
<p>Output:</p>
<pre><code>input tokens
[ 101 4074 3268 1999 2414  102]
predicted tags
[&#39;O&#39;, &#39;B-per&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-geo&#39;, &#39;O&#39;]
</code></pre>
<p>You can run this code with an NER dataset in required format, in <a href="https://www.kaggle.com/nandanapoorv/entity-recognition-with-tf-keras-and-huggingface">this</a> kaggle kernel. Please enable TPU for faster training.</p>
a:[["$","div",null,{"className":"mb-7 font-light","children":["↖ ",["$","a",null,{"rel":"","target":"","className":" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400","href":"/blog","children":"Back"}]]}],["$","article",null,{"className":"__className_bfc3c1 leading-7 antialiased","children":[["$","h1",null,{"children":"Entity Recognition with BERT"}],["$","p",null,{"className":"text-neutral-500","children":"2 August 2020"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]]}],["$","$Lc",null,{}]]
