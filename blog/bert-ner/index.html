<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="font" href="/_next/static/media/2aaf0723e720e8b9-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/b89f66ecdb077e7f-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/494afaf338ab5c7d.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7c9a1029ba1c2cf7.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-9747723084e35360.js" as="script" fetchPriority="low"/><script src="/_next/static/chunks/fd9d1056-a99b58d3cc150217.js" async=""></script><script src="/_next/static/chunks/596-ff6b9f15ce906e24.js" async=""></script><script src="/_next/static/chunks/main-app-4f41eba75df82bd8.js" async=""></script><title>Entity Recognition with BERT</title><meta name="description" content="Using tf.keras and huggingface for NER"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="bg-[#1a1a1a] text-neutral-200 __className_20951f"><main class="flex min-h-screen flex-col items-center py-16 md:py-32 px-6"><div class="max-w-lg w-full flex flex-col"><div class="mb-7 font-light">â†– <a rel="" target="" class=" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400" href="/blog">Back</a></div><article class="__className_bfc3c1 leading-7 antialiased"><h1>Entity Recognition with BERT</h1><p class="text-neutral-500">2 August 2020</p><div><h2>Introduction</h2>
<p>This post uses BERT (from huggingface) and tf.keras to train a NER model.
The data is expected in this format:</p>
<pre><code>    Sentence #	 Word            Tag
0   Sentence: 1	 Thousands       O
1   NaN	         of   	        O
2   NaN	         demonstrators  O
3   NaN	         have           O
4   NaN	         marched        O
5   NaN	         through        O
6   NaN	         London         B-geo
7   NaN	         to             O
8   NaN	         protest        O
...
...
24  Sentence: 2	 Families       O
25  NaN	         of             O
26  NaN	         soldiers       O
...
</code></pre>
<h2>Setup</h2>
<pre><code class="language-python">import os
import re
import json
import string
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn import preprocessing
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig

max_len = 384
configuration = BertConfig()
data_csv = &quot;ner_dataset.csv&quot;
</code></pre>
<h2>Setup Tokenizers</h2>
<pre><code class="language-python"># Save the slow pretrained tokenizer
slow_tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
save_path = &quot;bert_base_uncased/&quot;
if not os.path.exists(save_path):
    os.makedirs(save_path)
slow_tokenizer.save_pretrained(save_path)

# Load the fast tokenizer from saved file
tokenizer = BertWordPieceTokenizer(&quot;bert_base_uncased/vocab.txt&quot;, lowercase=True)
</code></pre>
<h2>Define model</h2>
<p><strong>Model</strong><br>Add a fully connected layer that takes token embeddings from BERT as input and<br>predicts probability of that token belonging to each of the possible tags.<br>The fully connected layer in the code below shows a <code>keras.layers.Dense</code> layer<br>with <code>num_tags+1</code> units to accomodate a padding label.</p>
<p><strong>Masked Loss</strong><br>Each batch of data will consist of variable sized sentence tokens with<br>appropriate padding in both input and target.<br>During loss calculation, we ignore the loss corresponding padding tokens<br>in the target.</p>
<pre><code class="language-python">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False, reduction=tf.keras.losses.Reduction.NONE
)

def masked_ce_loss(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 17))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)

def create_model(num_tags):
    ## BERT encoder
    encoder = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)

    ## NER Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)
    embedding = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask
    )[0]
    embedding = layers.Dropout(0.3)(embedding)
    tag_logits = layers.Dense(num_tags+1, activation=&#39;softmax&#39;)(embedding)
    
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[tag_logits],
    )
    optimizer = keras.optimizers.Adam(lr=3e-5)
    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=[&#39;accuracy&#39;])
    return model
</code></pre>
<h2>Preprocess dataset</h2>
<p>First, we read the convert the rows of our data file into sentences and lists of<br>tags. <code>sklearn.preprocessing.LabelEncoder</code> encodes each tag in a number.<br>Then, we create tokenize each sentence using BERT tokenizer from huggingface.<br>After tokenization each sentence is represented by a set of input_ids,<br>attention_masks and token_type_ids. Note that a single word may be tokenized into<br>multiple tokens. In that case, each token gets the label of the original word.<br>eg.</p>
<pre><code>this is   a    differentiator -&gt; [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;different&#39;, &#39;##ia&#39;, &#39;##tor&#39;]
tag1 tag2 tag3 tag4           -&gt; [tag1,   tag2, tag3, tag4,       tag4,   tag4]
</code></pre>
<pre><code class="language-python">def process_csv(data_path):
    df = pd.read_csv(data_path, encoding=&quot;latin-1&quot;)
    df.loc[:, &quot;Sentence #&quot;] = df[&quot;Sentence #&quot;].fillna(method=&quot;ffill&quot;)
    enc_tag = preprocessing.LabelEncoder()
    df.loc[:, &quot;Tag&quot;] = enc_tag.fit_transform(df[&quot;Tag&quot;])
    sentences = df.groupby(&quot;Sentence #&quot;)[&quot;Word&quot;].apply(list).values
    tag = df.groupby(&quot;Sentence #&quot;)[&quot;Tag&quot;].apply(list).values
    return sentences, tag, enc_tag


def create_inputs_targets(data_csv):
    dataset_dict = {
        &quot;input_ids&quot;: [],
        &quot;token_type_ids&quot;: [],
        &quot;attention_mask&quot;: [],
        &quot;tags&quot;: []
    }
    sentences, tags, tag_encoder = process_csv(data_csv)
    
    for sentence, tag in zip(sentences, tags):
        input_ids = []
        target_tags = []
        for idx, word in enumerate(sentence):
            ids = tokenizer.encode(word, add_special_tokens=False)
            input_ids.extend(ids.ids)
            num_tokens = len(ids)
            target_tags.extend([tag[idx]] * num_tokens)
        
        
        # Pad truncate
        input_ids = input_ids[:max_len - 2]
        target_tags = target_tags[:max_len - 2]

        input_ids = [101] + input_ids + [102]
        target_tags = [16] + target_tags + [16]
        token_type_ids = [0] * len(input_ids)
        attention_mask = [1] * len(input_ids)
        padding_len = max_len - len(input_ids)

        input_ids = input_ids + ([0] * padding_len)
        attention_mask = attention_mask + ([0] * padding_len)
        token_type_ids = token_type_ids + ([0] * padding_len)
        target_tags = target_tags + ([17] * padding_len)
        
        dataset_dict[&quot;input_ids&quot;].append(input_ids)
        dataset_dict[&quot;token_type_ids&quot;].append(token_type_ids)
        dataset_dict[&quot;attention_mask&quot;].append(attention_mask)
        dataset_dict[&quot;tags&quot;].append(target_tags)
        assert len(target_tags) == max_len, f&#39;{len(input_ids)}, {len(target_tags)}&#39;
        
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict[&quot;input_ids&quot;],
        dataset_dict[&quot;token_type_ids&quot;],
        dataset_dict[&quot;attention_mask&quot;],
    ]
    y = dataset_dict[&quot;tags&quot;]
    return x, y, tag_encoder
</code></pre>
<h2>Create model</h2>
<p>Use TPU if possible.</p>
<pre><code class="language-python">num_tags = pd.read_csv(data_csv, encoding=&quot;latin-1&quot;)[&quot;Tag&quot;].nunique()

use_tpu = None
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    use_tpu = True
except:
    use_tpu = False

if use_tpu:
    # Create distribution strategy
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)

    # Create model
    with strategy.scope():
        model = create_model(num_tags)
else:
    model = create_model(num_tags)
    
model.summary()
</code></pre>
<h1>Output:
```
Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     </h1>
<h1>input_1 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 384)]        0<br>__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]<br>__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 384, 768)     0           tf_bert_model[0][0]<br>__________________________________________________________________________________________________
dense (Dense)                   (None, 384, 18)      13842       dropout_37[0][0]                 </h1>
<p>Total params: 109,496,082
Trainable params: 109,496,082
Non-trainable params: 0</p>
<hr>
<pre><code>
## Train

```python
x_train, y_train, tag_encoder = create_inputs_targets(data_csv)

bs = 64 if use_tpu else 16

model.fit(
    x_train,
    y_train,
    epochs=1,
    verbose=1,
    batch_size=bs,
    validation_split=0.1
)
</code></pre>
<p>Output:</p>
<pre><code>675/675 [==============================] - 151s 223ms/step - accuracy: 0.9909 - loss: nan - val_accuracy: 0.9969 - val_loss: 2.3592
</code></pre>
<h2>Inference</h2>
<p>Check the predicted tags for a sample sentence.</p>
<pre><code class="language-python">def create_test_input_from_text(texts):
    dataset_dict = {
        &quot;input_ids&quot;: [],
        &quot;token_type_ids&quot;: [],
        &quot;attention_mask&quot;: []
    }
    for sentence in texts:
        input_ids = []
        for idx, word in enumerate(sentence.split()):
            ids = tokenizer.encode(word, add_special_tokens=False)
            input_ids.extend(ids.ids)
            num_tokens = len(ids)
            
        # Pad and create attention masks.
        # Skip if truncation is needed
        input_ids = input_ids[:max_len - 2]

        input_ids = [101] + input_ids + [102]
        n_tokens = len(input_ids)
        token_type_ids = [0] * len(input_ids)
        attention_mask = [1] * len(input_ids)
        padding_len = max_len - len(input_ids)

        input_ids = input_ids + ([0] * padding_len)
        attention_mask = attention_mask + ([0] * padding_len)
        token_type_ids = token_type_ids + ([0] * padding_len)
        
        dataset_dict[&quot;input_ids&quot;].append(input_ids)
        dataset_dict[&quot;token_type_ids&quot;].append(token_type_ids)
        dataset_dict[&quot;attention_mask&quot;].append(attention_mask)
        
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])

    x = [
        dataset_dict[&quot;input_ids&quot;],
        dataset_dict[&quot;token_type_ids&quot;],
        dataset_dict[&quot;attention_mask&quot;],
    ]
    return x, n_tokens

test_inputs = [&quot;alex lives in london&quot;]
x_test, n_tokens = create_test_input_from_text(test_inputs)
print(&#39;input tokens&#39;)
print(x_test[0][0][:n_tokens])
pred_test = model.predict(x_test)
pred_tags = np.argmax(pred_test,2)[0][:n_tokens]  # ignore predictions of padding tokens

# create dictionary of tags and and their indexes
le_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))
print(&#39;predicted tags&#39;)
print([le_dict.get(_, &#39;[pad]&#39;) for _ in pred_tags])
</code></pre>
<p>Output:</p>
<pre><code>input tokens
[ 101 4074 3268 1999 2414  102]
predicted tags
[&#39;O&#39;, &#39;B-per&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-geo&#39;, &#39;O&#39;]
</code></pre>
<p>You can run this code with an NER dataset in required format, in <a href="https://www.kaggle.com/nandanapoorv/entity-recognition-with-tf-keras-and-huggingface">this</a> kaggle kernel. Please enable TPU for faster training.</p>
</div></article></div></main><script src="/_next/static/chunks/webpack-9747723084e35360.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2aaf0723e720e8b9-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/494afaf338ab5c7d.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/media/b89f66ecdb077e7f-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/css/7c9a1029ba1c2cf7.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"6:I{\"id\":7948,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n8:I{\"id\":6628,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":7767,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"5"])</script><script>self.__next_f.push([1,"96:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\na:I{\"id\":7920,\"chunks\":[\"272:static/chunks/webpack-9747723084e35360.js\",\"971:static/chunks/fd9d1056-a99b58d3cc150217.js\",\"596:static/chunks/596-ff6b9f15ce906e24.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/494afaf338ab5c7d.css\",\"precedence\":\"next\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"KnsO8z-ENGbRNQP4VRChH\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/bert-ner/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"bert-ner\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"bert-ner\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L7\"],\"globalErrorComponent\":\"$8\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-[#1a1a1a] text-neutral-200 __className_20951f\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center py-16 md:py-32 px-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-lg w-full flex flex-col\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"bert-ner\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",\"$Lc\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"bert-ner\\\"}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7c9a1029ba1c2cf7.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"bert-ner\",\"d\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Entity Recognition with BERT\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Using tf.keras and huggingface for NER\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"d:T2d3f,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis post uses BERT (from huggingface) and tf.keras to train a NER model.\nThe data is expected in this format:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    Sentence #\t Word            Tag\n0   Sentence: 1\t Thousands       O\n1   NaN\t         of   \t        O\n2   NaN\t         demonstrators  O\n3   NaN\t         have           O\n4   NaN\t         marched        O\n5   NaN\t         through        O\n6   NaN\t         London         B-geo\n7   NaN\t         to             O\n8   NaN\t         protest        O\n...\n...\n24  Sentence: 2\t Families       O\n25  NaN\t         of             O\n26  NaN\t         soldiers       O\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSetup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport os\nimport re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn import preprocessing\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\nmax_len = 384\nconfiguration = BertConfig()\ndata_csv = \u0026quot;ner_dataset.csv\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSetup Tokenizers\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003e# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;)\nsave_path = \u0026quot;bert_base_uncased/\u0026quot;\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\u0026quot;bert_base_uncased/vocab.txt\u0026quot;, lowercase=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDefine model\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e\u003cbr\u003eAdd a fully connected layer that takes token embeddings from BERT as input and\u003cbr\u003epredicts probability of that token belonging to each of the possible tags.\u003cbr\u003eThe fully connected layer in the code below shows a \u003ccode\u003ekeras.layers.Dense\u003c/code\u003e layer\u003cbr\u003ewith \u003ccode\u003enum_tags+1\u003c/code\u003e units to accomodate a padding label.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMasked Loss\u003c/strong\u003e\u003cbr\u003eEach batch of data will consist of variable sized sentence tokens with\u003cbr\u003eappropriate padding in both input and target.\u003cbr\u003eDuring loss calculation, we ignore the loss corresponding padding tokens\u003cbr\u003ein the target.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n)\n\ndef masked_ce_loss(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 17))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\ndef create_model(num_tags):\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;)\n\n    ## NER Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n    embedding = layers.Dropout(0.3)(embedding)\n    tag_logits = layers.Dense(num_tags+1, activation=\u0026#39;softmax\u0026#39;)(embedding)\n    \n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[tag_logits],\n    )\n    optimizer = keras.optimizers.Adam(lr=3e-5)\n    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=[\u0026#39;accuracy\u0026#39;])\n    return model\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePreprocess dataset\u003c/h2\u003e\n\u003cp\u003eFirst, we read the convert the rows of our data file into sentences and lists of\u003cbr\u003etags. \u003ccode\u003esklearn.preprocessing.LabelEncoder\u003c/code\u003e encodes each tag in a number.\u003cbr\u003eThen, we create tokenize each sentence using BERT tokenizer from huggingface.\u003cbr\u003eAfter tokenization each sentence is represented by a set of input_ids,\u003cbr\u003eattention_masks and token_type_ids. Note that a single word may be tokenized into\u003cbr\u003emultiple tokens. In that case, each token gets the label of the original word.\u003cbr\u003eeg.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ethis is   a    differentiator -\u0026gt; [\u0026#39;this\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;different\u0026#39;, \u0026#39;##ia\u0026#39;, \u0026#39;##tor\u0026#39;]\ntag1 tag2 tag3 tag4           -\u0026gt; [tag1,   tag2, tag3, tag4,       tag4,   tag4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef process_csv(data_path):\n    df = pd.read_csv(data_path, encoding=\u0026quot;latin-1\u0026quot;)\n    df.loc[:, \u0026quot;Sentence #\u0026quot;] = df[\u0026quot;Sentence #\u0026quot;].fillna(method=\u0026quot;ffill\u0026quot;)\n    enc_tag = preprocessing.LabelEncoder()\n    df.loc[:, \u0026quot;Tag\u0026quot;] = enc_tag.fit_transform(df[\u0026quot;Tag\u0026quot;])\n    sentences = df.groupby(\u0026quot;Sentence #\u0026quot;)[\u0026quot;Word\u0026quot;].apply(list).values\n    tag = df.groupby(\u0026quot;Sentence #\u0026quot;)[\u0026quot;Tag\u0026quot;].apply(list).values\n    return sentences, tag, enc_tag\n\n\ndef create_inputs_targets(data_csv):\n    dataset_dict = {\n        \u0026quot;input_ids\u0026quot;: [],\n        \u0026quot;token_type_ids\u0026quot;: [],\n        \u0026quot;attention_mask\u0026quot;: [],\n        \u0026quot;tags\u0026quot;: []\n    }\n    sentences, tags, tag_encoder = process_csv(data_csv)\n    \n    for sentence, tag in zip(sentences, tags):\n        input_ids = []\n        target_tags = []\n        for idx, word in enumerate(sentence):\n            ids = tokenizer.encode(word, add_special_tokens=False)\n            input_ids.extend(ids.ids)\n            num_tokens = len(ids)\n            target_tags.extend([tag[idx]] * num_tokens)\n        \n        \n        # Pad truncate\n        input_ids = input_ids[:max_len - 2]\n        target_tags = target_tags[:max_len - 2]\n\n        input_ids = [101] + input_ids + [102]\n        target_tags = [16] + target_tags + [16]\n        token_type_ids = [0] * len(input_ids)\n        attention_mask = [1] * len(input_ids)\n        padding_len = max_len - len(input_ids)\n\n        input_ids = input_ids + ([0] * padding_len)\n        attention_mask = attention_mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        target_tags = target_tags + ([17] * padding_len)\n        \n        dataset_dict[\u0026quot;input_ids\u0026quot;].append(input_ids)\n        dataset_dict[\u0026quot;token_type_ids\u0026quot;].append(token_type_ids)\n        dataset_dict[\u0026quot;attention_mask\u0026quot;].append(attention_mask)\n        dataset_dict[\u0026quot;tags\u0026quot;].append(target_tags)\n        assert len(target_tags) == max_len, f\u0026#39;{len(input_ids)}, {len(target_tags)}\u0026#39;\n        \n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\u0026quot;input_ids\u0026quot;],\n        dataset_dict[\u0026quot;token_type_ids\u0026quot;],\n        dataset_dict[\u0026quot;attention_mask\u0026quot;],\n    ]\n    y = dataset_dict[\u0026quot;tags\u0026quot;]\n    return x, y, tag_encoder\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreate model\u003c/h2\u003e\n\u003cp\u003eUse TPU if possible.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003enum_tags = pd.read_csv(data_csv, encoding=\u0026quot;latin-1\u0026quot;)[\u0026quot;Tag\u0026quot;].nunique()\n\nuse_tpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    use_tpu = True\nexcept:\n    use_tpu = False\n\nif use_tpu:\n    # Create distribution strategy\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model(num_tags)\nelse:\n    model = create_model(num_tags)\n    \nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eOutput:\n```\nModel: \u0026quot;model\u0026quot;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \u003c/h1\u003e\n\u003ch1\u003einput_1 (InputLayer)            [(None, 384)]        0\u003cbr\u003e__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 384)]        0\u003cbr\u003e__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 384)]        0\u003cbr\u003e__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 384, 768), ( 109482240   input_1[0][0]\u003cbr\u003e__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 384, 768)     0           tf_bert_model[0][0]\u003cbr\u003e__________________________________________________________________________________________________\ndense (Dense)                   (None, 384, 18)      13842       dropout_37[0][0]                 \u003c/h1\u003e\n\u003cp\u003eTotal params: 109,496,082\nTrainable params: 109,496,082\nNon-trainable params: 0\u003c/p\u003e\n\u003chr\u003e\n\u003cpre\u003e\u003ccode\u003e\n## Train\n\n```python\nx_train, y_train, tag_encoder = create_inputs_targets(data_csv)\n\nbs = 64 if use_tpu else 16\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    verbose=1,\n    batch_size=bs,\n    validation_split=0.1\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOutput:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e675/675 [==============================] - 151s 223ms/step - accuracy: 0.9909 - loss: nan - val_accuracy: 0.9969 - val_loss: 2.3592\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eInference\u003c/h2\u003e\n\u003cp\u003eCheck the predicted tags for a sample sentence.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef create_test_input_from_text(texts):\n    dataset_dict = {\n        \u0026quot;input_ids\u0026quot;: [],\n        \u0026quot;token_type_ids\u0026quot;: [],\n        \u0026quot;attention_mask\u0026quot;: []\n    }\n    for sentence in texts:\n        input_ids = []\n        for idx, word in enumerate(sentence.split()):\n            ids = tokenizer.encode(word, add_special_tokens=False)\n            input_ids.extend(ids.ids)\n            num_tokens = len(ids)\n            \n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        input_ids = input_ids[:max_len - 2]\n\n        input_ids = [101] + input_ids + [102]\n        n_tokens = len(input_ids)\n        token_type_ids = [0] * len(input_ids)\n        attention_mask = [1] * len(input_ids)\n        padding_len = max_len - len(input_ids)\n\n        input_ids = input_ids + ([0] * padding_len)\n        attention_mask = attention_mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        \n        dataset_dict[\u0026quot;input_ids\u0026quot;].append(input_ids)\n        dataset_dict[\u0026quot;token_type_ids\u0026quot;].append(token_type_ids)\n        dataset_dict[\u0026quot;attention_mask\u0026quot;].append(attention_mask)\n        \n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\u0026quot;input_ids\u0026quot;],\n        dataset_dict[\u0026quot;token_type_ids\u0026quot;],\n        dataset_dict[\u0026quot;attention_mask\u0026quot;],\n    ]\n    return x, n_tokens\n\ntest_inputs = [\u0026quot;alex lives in london\u0026quot;]\nx_test, n_tokens = create_test_input_from_text(test_inputs)\nprint(\u0026#39;input tokens\u0026#39;)\nprint(x_test[0][0][:n_tokens])\npred_test = model.predict(x_test)\npred_tags = np.argmax(pred_test,2)[0][:n_tokens]  # ignore predictions of padding tokens\n\n# create dictionary of tags and and their indexes\nle_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))\nprint(\u0026#39;predicted tags\u0026#39;)\nprint([le_dict.get(_, \u0026#39;[pad]\u0026#39;) for _ in pred_tags])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOutput:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003einput tokens\n[ 101 4074 3268 1999 2414  102]\npredicted tags\n[\u0026#39;O\u0026#39;, \u0026#39;B-per\u0026#39;, \u0026#39;O\u0026#39;, \u0026#39;O\u0026#39;, \u0026#39;B-geo\u0026#39;, \u0026#39;O\u0026#39;]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can run this code with an NER dataset in required format, in \u003ca href=\"https://www.kaggle.com/nandanapoorv/entity-recognition-with-tf-keras-and-huggingface\"\u003ethis\u003c/a\u003e kaggle kernel. Please enable TPU for faster training.\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"div\",null,{\"className\":\"mb-7 font-light\",\"children\":[\"â†– \",[\"$\",\"a\",null,{\"rel\":\"\",\"target\":\"\",\"className\":\" text-neutral-200 underline underline-offset-2 transition-all duration-200 decoration-neutral-600 hover:decoration-neutral-400\",\"href\":\"/blog\",\"children\":\"Back\"}]]}],[\"$\",\"article\",null,{\"className\":\"__className_bfc3c1 leading-7 antialiased\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Entity Recognition with BERT\"}],[\"$\",\"p\",null,{\"className\":\"text-neutral-500\",\"children\":\"2 August 2020\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$d\"}}]]}]]\n"])</script></body></html>